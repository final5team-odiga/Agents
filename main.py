import os
import json
import EssayAgents
from dotenv import load_dotenv
from read_txt_storage import read_all_txt_files_from_userdata, connect_str, container_name
from openai import AzureOpenAI
from typing import Any, Dict, List, Optional, Union
from pathlib import Path
dotenv_path = Path(
    r'C:/Users/EL0027/OneDrive/Desktop/MSfinalproject/crewAI_pratice/.env')
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=dotenv_path, override=True)
# âœ… BaseLLM í´ë˜ìŠ¤ ì§ì ‘ ì •ì˜


class BaseLLM:
    def __init__(self, model: Optional[str] = None):
        self.model = model

    def call(self, messages: Union[str, List[Dict[str, str]]]) -> Union[str, Any]:
        raise NotImplementedError("call() ë©”ì„œë“œëŠ” ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

    def supports_function_calling(self) -> bool:
        return False

    def supports_stop_words(self) -> bool:
        return False

    def get_context_window_size(self) -> int:
        return 2048  # ê¸°ë³¸ê°’


class AzureOpenAILLM(BaseLLM):
    """Azure OpenAI APIë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ LLM í´ë˜ìŠ¤"""

    def __init__(self):
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ë° ê°€ì ¸ì˜¤ê¸°
        self.api_key = os.getenv("AZURE_API_KEY")
        self.azure_endpoint = os.getenv("AZURE_API_BASE")
        self.api_version = os.getenv("AZURE_API_VERSION")
        self.deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
        if not self.api_key:
            raise ValueError("AZURE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not self.azure_endpoint:
            raise ValueError("AZURE_API_BASE í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not self.api_version:
            raise ValueError("AZURE_API_VERSION í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not self.deployment_name:
            raise ValueError("AZURE_OPENAI_DEPLOYMENT_NAME í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” - model ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
        super().__init__(model=f"azure/{self.deployment_name}")
        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = AzureOpenAI(
            api_key=self.api_key,
            azure_endpoint=self.azure_endpoint,
            api_version=self.api_version
        )

    def call(
        self,
        messages: Union[str, List[Dict[str, str]]],
        tools: Optional[List[dict]] = None,
        callbacks: Optional[List[Any]] = None,
        available_functions: Optional[Dict[str, Any]] = None,
    ) -> Union[str, Any]:
        """LLMì— ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."""
        try:
            # ë¬¸ìì—´ ë©”ì‹œì§€ë¥¼ ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if isinstance(messages, str):
                messages = [{"role": "user", "content": messages}]
            # ë„êµ¬(í•¨ìˆ˜ í˜¸ì¶œ) ì§€ì› ì—¬ë¶€ì— ë”°ë¼ ìš”ì²­ êµ¬ì„±
            if tools and self.supports_function_calling():
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=messages,
                    tools=tools,
                    temperature=0.7,
                    max_tokens=4000
                )
            else:
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=4000
                )
            # í•¨ìˆ˜ í˜¸ì¶œ ì‘ë‹µ ì²˜ë¦¬
            if (
                tools
                and self.supports_function_calling()
                and response.choices[0].message.tool_calls
                and available_functions
            ):
                tool_call = response.choices[0].message.tool_calls[0]
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                if function_name in available_functions:
                    function_to_call = available_functions[function_name]
                    function_response = function_to_call(**function_args)
                    # í•¨ìˆ˜ ì‘ë‹µì„ í¬í•¨í•˜ì—¬ í›„ì† ìš”ì²­
                    messages.append(response.choices[0].message)
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": function_name,
                        "content": str(function_response)
                    })
                    second_response = self.client.chat.completions.create(
                        model=self.deployment_name,
                        messages=messages,
                        temperature=0.7,
                        max_tokens=4000
                    )
                    return second_response.choices[0].message.content
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ë°˜í™˜
            return response.choices[0].message.content
        except Exception as e:
            # ì˜¤ë¥˜ ë¡œê¹… ë° ì˜ˆì™¸ ë°œìƒ
            print(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            raise RuntimeError(f"LLM ìš”ì²­ ì‹¤íŒ¨: {str(e)}")

    def supports_function_calling(self) -> bool:
        """í•¨ìˆ˜ í˜¸ì¶œ ì§€ì› ì—¬ë¶€ í™•ì¸"""
        # GPT-4, GPT-4 Turbo, GPT-4o ëª¨ë¸ì€ í•¨ìˆ˜ í˜¸ì¶œì„ ì§€ì›í•©ë‹ˆë‹¤
        return True

    def supports_stop_words(self) -> bool:
        """ì¤‘ì§€ ë‹¨ì–´ ì§€ì› ì—¬ë¶€ í™•ì¸"""
        return True

    def get_context_window_size(self) -> int:
        """LLMì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° ë°˜í™˜"""
        # ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ ê°’ ë°˜í™˜
        return 8192  # ê¸°ë³¸ê°’, í•„ìš”ì— ë”°ë¼ ì¡°ì •


def get_azure_llm():
    """Azure OpenAI LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return AzureOpenAILLM()


# ì´ íŒŒì¼ ë§¨ ì•„ë˜ì— ì¶”ê°€
if __name__ == "__main__":
    llm = get_azure_llm()
    agents = EssayAgents.get_essay_agents()
    manager = EssayAgents.EssayAgentManager(agents)

    # 1) txt íŒŒì¼ë“¤ ì½ì–´ì˜¤ê¸°
    txt_contents = read_all_txt_files_from_userdata(
        connect_str, container_name)

   # âœ… ë””ë²„ê¹…ìš©: ê° íŒŒì¼ ì´ë¦„ê³¼ ë‚´ìš©ì„ ì¶œë ¥
    print("ğŸ“‚ ë¶ˆëŸ¬ì˜¨ í…ìŠ¤íŠ¸ íŒŒì¼ ëª©ë¡:")
    for filename, content in txt_contents.items():
        print(f"\nğŸ“„ íŒŒì¼ëª…: {filename}")

    # user_input = "\n\n".join(txt_contents.values())
    user_input = {
        "EssayFormatAgent1": txt_contents.get("ì—¬í–‰ì„ ë– ë‚œ ì´ìœ , ë‚˜ì˜ ê°ì •ì€?", ""),
        "EssayFormatAgent2": txt_contents.get("{ê³„ì ˆ}ì— {ë„ì‹œ}ë¥¼ ì°¾ì€ ì´ìœ ê°€ ìˆë‹¤ë©´?", ""),
        "EssayFormatAgent3": txt_contents.get("{ì´ë¦„}ë‹˜ì´ ê·¸ê³³ì—ì„œì˜ ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë–»ê²Œ í˜ëŸ¬ê°”ëŠ”ì§€ ê¶ê¸ˆí•´ì§„ë‹¤. ì–´ë• ëŠ”ì§€", ""),
        "EssayFormatAgent4": txt_contents.get("3í˜ì´ì§€ ë‚´ìš©ì˜ ì—°ì¥ì„ ì¸ í•˜ë£¨ ë§ˆë¬´ë¦¬", ""),
    }

    # 3) ê° ì—ì´ì „íŠ¸ë³„ë¡œ ì •ì œ ì‘ì—… ì‹¤í–‰
    results = manager.run_all(user_input, llm=llm)

    # 4) ê²°ê³¼ ì¶œë ¥
    for agent_name, essay_text in results.items():
        print(f"\n[{agent_name}] ê²°ê³¼:\n{essay_text}\n")

    # 5) ê²°ê³¼ ì €ì¥
    output_dir = "EssayAgents_result"
    os.makedirs(output_dir, exist_ok=True)

    for agent_name, essay_text in results.items():
        filename = os.path.join(output_dir, f"{agent_name}_result.txt")
        with open(filename, "w", encoding="utf-8") as f:
            f.write(essay_text)

    print(f"\nâœ… ëª¨ë“  ê²°ê³¼ê°€ '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
