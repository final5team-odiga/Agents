import asyncio
import sys
import time
import concurrent.futures
from typing import Dict, List, Optional, Callable, Any
from collections import deque
from dataclasses import dataclass
import os
import json
import re
from crewai import Agent, Task, Crew, Process
from custom_llm import get_azure_llm
from utils.hybridlogging import get_hybrid_logger
from utils.ai_search_isolation import CoordinatorAgentIsolation
from utils.session_isolation import SessionAwareMixin
from utils.agent_communication_isolation import InterAgentCommunicationMixin


@dataclass
class WorkItem:
    id: str
    task_func: Callable
    args: tuple
    kwargs: dict
    priority: int = 0
    max_retries: int = 3
    current_retry: int = 0
    timeout: float = 300.0

class AsyncWorkQueue:
    def __init__(self, max_workers: int = 2, max_queue_size: int = 50):
        self.max_workers = max_workers
        self.max_queue_size = max_queue_size
        self.work_queue = deque()
        self.active_tasks = {}
        self.results = {}
        self.semaphore = asyncio.Semaphore(max_workers)
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)

    async def add_work(self, work_item: WorkItem) -> str:
        """ì‘ì—…ì„ íì— ì¶”ê°€"""
        if len(self.work_queue) >= self.max_queue_size:
            old_item = self.work_queue.popleft()
            print(f"âš ï¸ í ìš©ëŸ‰ ì´ˆê³¼ë¡œ ì‘ì—… {old_item.id} ì œê±°")
        
        self.work_queue.append(work_item)
        return work_item.id

    async def process_work_item(self, work_item: WorkItem) -> Optional[Any]:
        """ê°œë³„ ì‘ì—… ì²˜ë¦¬"""
        async with self.semaphore:
            try:
                print(f"ğŸ”„ ì‘ì—… {work_item.id} ì‹œì‘ (ì‹œë„ {work_item.current_retry + 1}/{work_item.max_retries + 1})")
                
                # ìˆ˜ì •: ì½”ë£¨í‹´ ê°ì²´ì™€ ì½”ë£¨í‹´ í•¨ìˆ˜ êµ¬ë¶„
                if asyncio.iscoroutine(work_item.task_func):
                    # ì´ë¯¸ ìƒì„±ëœ ì½”ë£¨í‹´ ê°ì²´
                    result = await asyncio.wait_for(work_item.task_func, timeout=work_item.timeout)
                elif asyncio.iscoroutinefunction(work_item.task_func):
                    # ì½”ë£¨í‹´ í•¨ìˆ˜
                    result = await asyncio.wait_for(
                        work_item.task_func(*work_item.args, **work_item.kwargs),
                        timeout=work_item.timeout
                    )
                elif callable(work_item.task_func):
                    # ì¼ë°˜ í˜¸ì¶œ ê°€ëŠ¥ ê°ì²´
                    result = await asyncio.wait_for(
                        asyncio.get_event_loop().run_in_executor(
                            self.executor,
                            lambda: work_item.task_func(*work_item.args, **work_item.kwargs)
                        ),
                        timeout=work_item.timeout
                    )
                else:
                    # í˜¸ì¶œ ë¶ˆê°€ëŠ¥í•œ ê°ì²´ì¸ ê²½ìš° ì˜¤ë¥˜ ë°œìƒ
                    raise TypeError(f"task_func is not callable: {type(work_item.task_func)}")
                
                self.results[work_item.id] = {"status": "success", "result": result}
                print(f"âœ… ì‘ì—… {work_item.id} ì™„ë£Œ")
                return result
                
            except asyncio.TimeoutError:
                print(f"â° ì‘ì—… {work_item.id} íƒ€ì„ì•„ì›ƒ ({work_item.timeout}ì´ˆ)")
                if work_item.current_retry < work_item.max_retries:
                    work_item.current_retry += 1
                    work_item.timeout *= 1.5
                    await self.add_work(work_item)
                else:
                    self.results[work_item.id] = {"status": "timeout", "error": "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼"}
                return None
                
            except Exception as e:
                print(f"âŒ ì‘ì—… {work_item.id} ì‹¤íŒ¨: {e}")
                if work_item.current_retry < work_item.max_retries:
                    work_item.current_retry += 1
                    await self.add_work(work_item)
                else:
                    self.results[work_item.id] = {"status": "error", "error": str(e)}
                return None
            
    async def process_queue(self) -> dict:
        """íì˜ ëª¨ë“  ì‘ì—…ì„ ë°°ì¹˜ ì²˜ë¦¬"""
        tasks = []
        while self.work_queue:
            work_item = self.work_queue.popleft()
            task = asyncio.create_task(self.process_work_item(work_item))
            tasks.append(task)
            self.active_tasks[work_item.id] = task
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
        
        return self.results

class CircuitBreaker:
    def __init__(self, failure_threshold: int = 10, recovery_timeout: float = 120.0):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

    def is_open(self) -> bool:
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = "HALF_OPEN"
                return False
            return True
        return False

    def record_success(self):
        self.failure_count = 0
        self.state = "CLOSED"

    def record_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"





TIMEOUT_CONFIGS = {
    'org_agent': 900,      # 15ë¶„
    'binding_agent': 1200, # 20ë¶„  
    'coordinator_agent': 600, # 10ë¶„
    'vector_init': 600,    # 10ë¶„
    'crew_execution': 900  # 15ë¶„
}


class CoordinatorAgent(CoordinatorAgentIsolation, SessionAwareMixin, InterAgentCommunicationMixin):
    """í†µí•© ì¡°ìœ¨ì (AI Search ê²©ë¦¬ ê°•í™”)"""
    
    def __init__(self, session_id: Optional[str] = None):
        # ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ
        self.llm = get_azure_llm()
        self.logger = get_hybrid_logger()
        self.crew_agent = self._create_crew_agent()
        self.text_analyzer_agent = self._create_text_analyzer_agent()
        self.image_analyzer_agent = self._create_image_analyzer_agent()
        self.work_queue = AsyncWorkQueue(max_workers=1, max_queue_size=20)
        self.circuit_breaker = CircuitBreaker()
        self.recursion_threshold = 600
        self.fallback_to_sync = False
        self.batch_size = 2
        self.execution_stats = {
            "total_attempts": 0,
            "successful_executions": 0,
            "fallback_used": 0,
            "circuit_breaker_triggered": 0,
            "timeout_occurred": 0
        }

        self.__init_isolation__()

        self.__init_session_awareness__(session_id)

        self.__init_inter_agent_communication__()

    async def _process_enhanced_crew_result_async(self, crew_result, extracted_text_data: Dict,
                                                extracted_image_data: Dict, org_results: List[Dict],
                                                binding_results: List[Dict]) -> Dict:
        """ê°•í™”ëœ Crew ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬ (ê²©ë¦¬ ê°•í™”)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._process_enhanced_crew_result_with_isolation, crew_result, extracted_text_data,
            extracted_image_data, org_results, binding_results
        )

    def _process_enhanced_crew_result_with_isolation(self, crew_result, extracted_text_data: Dict,
                                                   extracted_image_data: Dict, org_results: List[Dict],
                                                   binding_results: List[Dict]) -> Dict:
        """Crew ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬ ë° ê²©ë¦¬ ê°•í™”"""
        try:
            # 1. Azure AI Search ì˜í–¥ ì°¨ë‹¨ (ê°•í™”ëœ ê²©ë¦¬)
            parsed_data = self.block_azure_search_influence(crew_result)
            
            # 2. ê¸°ì¡´ ì²˜ë¦¬ ë¡œì§
            if not parsed_data.get('content_sections') or len(parsed_data.get('content_sections', [])) == 0:
                parsed_data = self._create_enhanced_structure_isolated(
                    extracted_text_data, extracted_image_data, org_results, binding_results
                )
            else:
                parsed_data = self._enhance_parsed_data_with_real_images_isolated(
                    parsed_data, extracted_image_data
                )
            
            # 3. ì½˜í…ì¸  ì§„ì •ì„± ê²€ì¦ ë° êµì • ì ìš© (ê°•í™”)
            parsed_data = self.validate_content_authenticity(parsed_data)
            
            # 4. ìµœì¢… ê²©ë¦¬ ê²€ì¦
            parsed_data = self._final_isolation_validation(parsed_data)
            
            # 5. ìµœì¢… ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            parsed_data['integration_metadata'] = {
                **parsed_data.get('integration_metadata', {}),
                "total_sections": len(parsed_data.get('content_sections', [])),
                "azure_search_influence_blocked": True,
                "original_content_validation_applied": True,
                "data_source_priority": "magazine_content_json_primary",
                "ai_search_isolation": {
                    "isolation_applied": True,
                    "contamination_report": self._get_isolation_report(),
                    "final_validation_passed": True
                }
            }
            
            return parsed_data
            
        except Exception as e:
            print(f"âš ï¸ Crew ê²°ê³¼ ì²˜ë¦¬ ë° ê²©ë¦¬ ì‹¤íŒ¨: {e}")
            return self._restore_from_magazine_content()

    def _create_enhanced_structure_isolated(self, extracted_text_data: Dict, extracted_image_data: Dict,
                                          org_results: List[Dict], binding_results: List[Dict]) -> Dict:
        """ê²©ë¦¬ëœ ê°•í™” êµ¬ì¡° ìƒì„±"""
        # ê¸°ì¡´ _create_enhanced_structure ë¡œì§ì— ê²©ë¦¬ ì ìš©
        content_sections = []
        selected_templates = []
        
        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì„¹ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¡° ìƒì„± (ê²©ë¦¬ ì ìš©)
        for i, section in enumerate(extracted_text_data.get('sections', [])):
            # ì„¹ì…˜ ì˜¤ì—¼ ê²€ì‚¬
            if self.isolation_manager.is_contaminated(section, f"extracted_section_{i}"):
                print(f"ğŸš« ì„¹ì…˜ {i+1} ì˜¤ì—¼ ê°ì§€, ê±´ë„ˆëœ€")
                continue
            
            template = section.get('template', f"Section{i+1:02d}.jsx")
            
            # í•´ë‹¹ í…œí”Œë¦¿ì˜ ê²©ë¦¬ëœ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            template_images = extracted_image_data.get('template_images', {}).get(template, [])
            clean_images = [img for img in template_images if self.isolation_manager.is_trusted_image_url(img)]
            
            # ì„¹ì…˜ êµ¬ì¡° ìƒì„±
            section_data = {
                "template": template,
                "title": section.get('title', ''),
                "subtitle": section.get('subtitle', ''),
                "body": section.get('body', ''),
                "tagline": section.get('tagline', 'TRAVEL & CULTURE'),
                "images": clean_images[:self.isolation_manager.config.max_images_per_section],
                "metadata": {
                    "content_quality": self._calculate_content_quality(section),
                    "image_count": len(clean_images[:self.isolation_manager.config.max_images_per_section]),
                    "source": section.get('layout_source', 'extracted'),
                    "real_content": True,
                    "fallback_used": False,
                    "ai_search_isolation": {
                        "isolation_applied": True,
                        "images_filtered": len(template_images) - len(clean_images),
                        "contamination_detected": False
                    }
                }
            }
            
            content_sections.append(section_data)
            selected_templates.append(template)
        
        if not content_sections:
            content_sections = [{
                "template": "Section01.jsx",
                "title": "ì—¬í–‰ ë§¤ê±°ì§„",
                "subtitle": "íŠ¹ë³„í•œ ì´ì•¼ê¸°",
                "body": "ê²©ë¦¬ ì‹œìŠ¤í…œì— ì˜í•´ ëª¨ë“  ì„¹ì…˜ì´ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "tagline": "TRAVEL & CULTURE",
                "images": [],
                "metadata": {
                    "content_quality": 0.5,
                    "image_count": 0,
                    "source": "isolated_fallback",
                    "real_content": False,
                    "fallback_used": True,
                    "ai_search_isolation": {
                        "isolation_applied": True,
                        "all_sections_filtered": True,
                        "reason": "contamination_detected"
                    }
                }
            }]
            selected_templates = ["Section01.jsx"]
        
        return {
            "selected_templates": selected_templates,
            "content_sections": content_sections,
            "integration_metadata": {
                "source": "enhanced_structure_isolated",
                "total_sections": len(content_sections),
                "azure_search_influence": "blocked",
                "content_authenticity": "verified",
                "ai_search_isolation": {
                    "isolation_applied": True,
                    "structure_created": True
                }
            }
        }

    def _enhance_parsed_data_with_real_images_isolated(self, parsed_data: Dict, extracted_image_data: Dict) -> Dict:
        """ê²©ë¦¬ëœ ì‹¤ì œ ì´ë¯¸ì§€ë¡œ íŒŒì‹±ëœ ë°ì´í„° ê°•í™”"""
        enhanced_sections = []
        
        for section in parsed_data.get('content_sections', []):
            template = section.get('template', '')
            
            # í…œí”Œë¦¿ë³„ ê²©ë¦¬ëœ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            template_images = extracted_image_data.get('template_images', {}).get(template, [])
            clean_images = [img for img in template_images if self.isolation_manager.is_trusted_image_url(img)]
            
            # ì„¹ì…˜ ê°•í™”
            enhanced_section = {
                **section,
                "images": clean_images[:self.isolation_manager.config.max_images_per_section],
                "metadata": {
                    **section.get('metadata', {}),
                    "image_count": len(clean_images[:self.isolation_manager.config.max_images_per_section]),
                    "real_images_applied": True,
                    "ai_search_isolation": {
                        "isolation_applied": True,
                        "images_filtered": len(template_images) - len(clean_images),
                        "trusted_images_only": True
                    }
                }
            }
            
            enhanced_sections.append(enhanced_section)
        
        parsed_data['content_sections'] = enhanced_sections
        return parsed_data

    def _final_isolation_validation(self, parsed_data: Dict) -> Dict:
        """ìµœì¢… ê²©ë¦¬ ê²€ì¦"""
        validated_sections = []
        
        for section in parsed_data.get('content_sections', []):
            # ì„¹ì…˜ ì „ì²´ ì˜¤ì—¼ ê²€ì‚¬
            if self.isolation_manager.is_contaminated(section, "final_validation"):
                print(f"ğŸš« ìµœì¢… ê²€ì¦ì—ì„œ ì„¹ì…˜ ì˜¤ì—¼ ê°ì§€: {section.get('title', 'unknown')[:30]}...")
                
                # ì›ë³¸ ë°ì´í„°ë¡œ ë³µì› ì‹œë„
                restored_section = self._restore_section_from_original(section)
                validated_sections.append(restored_section)
            else:
                validated_sections.append(section)
        
        parsed_data['content_sections'] = validated_sections
        parsed_data['final_isolation_validation'] = {
            "validation_applied": True,
            "sections_validated": len(validated_sections),
            "contamination_report": self._get_isolation_report()
        }
        
        return parsed_data

    def _restore_section_from_original(self, contaminated_section: Dict) -> Dict:
        """ì˜¤ì—¼ëœ ì„¹ì…˜ì„ ì›ë³¸ ë°ì´í„°ë¡œ ë³µì›"""
        try:
            magazine_content_path = "./output/magazine_content.json"
            if os.path.exists(magazine_content_path):
                with open(magazine_content_path, 'r', encoding='utf-8') as f:
                    original_data = json.load(f)
                
                # í•´ë‹¹ í…œí”Œë¦¿ì˜ ì›ë³¸ ë°ì´í„° ì°¾ê¸°
                template = contaminated_section.get('template', '')
                for section in original_data.get('sections', []):
                    if section.get('template') == template:
                        return {
                            "template": template,
                            "title": section.get('title', ''),
                            "subtitle": section.get('subtitle', ''),
                            "body": section.get('content', section.get('body', '')),
                            "tagline": "TRAVEL & CULTURE",
                            "images": contaminated_section.get('images', []),  # ì´ë¯¸ì§€ëŠ” ìœ ì§€
                            "metadata": {
                                **contaminated_section.get('metadata', {}),
                                "source": "magazine_content_json_restored",
                                "contamination_restored": True,
                                "ai_search_isolation": {
                                    "isolation_applied": True,
                                    "restored_from_original": True
                                }
                            }
                        }
        except Exception as e:
            print(f"âš ï¸ ì„¹ì…˜ ë³µì› ì‹¤íŒ¨: {e}")
        
        # ë³µì› ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            **contaminated_section,
            "title": "ì—¬í–‰ ì´ì•¼ê¸°",
            "subtitle": "íŠ¹ë³„í•œ ìˆœê°„ë“¤",
            "body": "ì›ë³¸ ë°ì´í„°ë¡œ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "metadata": {
                **contaminated_section.get('metadata', {}),
                "restoration_failed": True,
                "ai_search_isolation": {
                    "isolation_applied": True,
                    "restoration_attempted": True,
                    "restoration_failed": True
                }
            }
        }

    def _get_fallback_result(self, task_id: str) -> dict:
        """í´ë°± ê²°ê³¼ ìƒì„± (ê²©ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€)"""
        return {
            "selected_templates": ["Section01.jsx"],
            "content_sections": [{
                "template": "Section01.jsx",
                "title": "ì—¬í–‰ ë§¤ê±°ì§„",
                "subtitle": "íŠ¹ë³„í•œ ì´ì•¼ê¸°",
                "body": "Circuit Breaker ë˜ëŠ” ì‹¤íŒ¨ë¡œ ì¸í•œ í´ë°± ì½˜í…ì¸ ì…ë‹ˆë‹¤.",
                "tagline": "TRAVEL & CULTURE",
                "images": [],
                "metadata": {
                    "content_quality": 0.5,
                    "image_count": 0,
                    "source": "coordinator_fallback",
                    "real_content": False,
                    "fallback_used": True,
                    "ai_search_isolation": {
                        "isolation_applied": True,
                        "fallback_reason": "circuit_breaker_or_failure"
                    }
                }
            }],
            "integration_metadata": {
                "source": "fallback",
                "total_sections": 1,
                "azure_search_influence": "blocked",
                "content_authenticity": "fallback",
                "ai_search_isolation": {
                    "isolation_applied": True,
                    "fallback_used": True
                }
            }
        }

    def _check_recursion_depth(self):
        """í˜„ì¬ ì¬ê·€ ê¹Šì´ í™•ì¸"""
        frame = sys._getframe()
        depth = 0
        while frame:
            depth += 1
            frame = frame.f_back
        return depth

    def _should_use_sync(self):
        """ë™ê¸° ëª¨ë“œë¡œ ì „í™˜í• ì§€ íŒë‹¨"""
        current_depth = self._check_recursion_depth()
        if current_depth > self.recursion_threshold:
            print(f"âš ï¸ CoordinatorAgent ì¬ê·€ ê¹Šì´ {current_depth} ê°ì§€ - ë™ê¸° ëª¨ë“œë¡œ ì „í™˜")
            self.fallback_to_sync = True
            return True
        return self.fallback_to_sync



    async def execute_with_resilience(self, task_func: Callable, task_id: str,
                                  timeout: float = None, max_retries: int = 2,
                                  *args, **kwargs) -> Any:
        if timeout is None:
            for task_type, default_timeout in TIMEOUT_CONFIGS.items():
                if task_type in task_id.lower():
                    timeout = default_timeout
                    break
            else:
                timeout = 300

        if self.circuit_breaker.is_open():
            print(f"ğŸš« Circuit Breaker ì—´ë¦¼ - ì‘ì—… {task_id} ê±´ë„ˆëœ€")
            return self._get_fallback_result(task_id)

        # ìˆ˜ì •: ì›ë˜ í•¨ìˆ˜ì™€ ì¸ìë¥¼ WorkItemì— ì „ë‹¬
        work_item = WorkItem(
            id=task_id,
            task_func=task_func,  # ì›ë˜ í•¨ìˆ˜ ì „ë‹¬
            args=args,            # ì›ë˜ ì¸ì ì „ë‹¬
            kwargs=kwargs,        # ì›ë˜ í‚¤ì›Œë“œ ì¸ì ì „ë‹¬
            timeout=timeout,
            max_retries=max_retries
        )

        await self.work_queue.add_work(work_item)
        processed_results = await self.work_queue.process_queue()
        result_info = processed_results.get(task_id)

        if result_info and result_info["status"] == "success":
            self.circuit_breaker.record_success()
            return result_info["result"]
        else:
            self.circuit_breaker.record_failure()
            if result_info:
                print(f"âš ï¸ ì‘ì—… {task_id} ìµœì¢… ì‹¤íŒ¨: {result_info.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            else:
                print(f"âš ï¸ ì‘ì—… {task_id}ì˜ ê²°ê³¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (í ì²˜ë¦¬ í›„).")
            return self._get_fallback_result(task_id)


    def _get_fallback_result(self, task_id: str) -> dict:
        """ê°œì„ ëœ í´ë°± ê²°ê³¼ ìƒì„±"""
        self.execution_stats["fallback_used"] += 1
        reason = task_id  # ê¸°ë³¸ì ìœ¼ë¡œ task_idë¥¼ reasonìœ¼ë¡œ ì‚¬ìš©
        
        if "_timeout" in task_id:
            reason = "timeout"
        elif "_exception" in task_id:
            reason = "exception"
        elif "_type_error" in task_id:
            reason = "type_error"
        
        return {
            "selected_templates": ["Section01.jsx"],
            "content_sections": [{
                "template": "Section01.jsx",
                "title": "ì—¬í–‰ ë§¤ê±°ì§„ (í´ë°±)",
                "subtitle": f"íŠ¹ë³„í•œ ì´ì•¼ê¸° ({reason})",
                "body": f"CoordinatorAgent ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ ({reason})ìœ¼ë¡œ ì¸í•œ í´ë°± ì½˜í…ì¸ ì…ë‹ˆë‹¤. Task ID: {task_id}",
                "tagline": "TRAVEL & CULTURE",
                "images": [],
                "metadata": {
                    "fallback_used": True,
                    "reason": reason,
                    "task_id": task_id
                }
            }],
            "integration_metadata": {
                "total_sections": 1,
                "integration_quality_score": 0.5,
                "fallback_mode": True
            }
        }

    def _create_crew_agent(self):
        """ë©”ì¸ ì¡°ìœ¨ ì—ì´ì „íŠ¸ ìƒì„±"""
        return Agent(
            role="ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ì ë° ìµœì¢… í’ˆì§ˆ ë³´ì¦ ì „ë¬¸ê°€",
            goal="magazine_content.jsonì˜ ì›ë³¸ í…ìŠ¤íŠ¸ì™€ image_analysis.jsonì˜ ì´ë¯¸ì§€ ë¶„ì„ ë°ì´í„°ë¥¼ í•„ìˆ˜ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì™„ë²½í•œ ë§¤ê±°ì§„ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³ , Azure AI Search ì™¸ë¶€ ë°ì´í„°ì˜ ì˜í–¥ì„ ìµœì†Œí™”í•œ ìˆœìˆ˜í•œ template_data.jsonì„ ì œê³µ",
            backstory="""ë‹¹ì‹ ì€ 25ë…„ê°„ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì¶œíŒì‚¬ì—ì„œ ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ë° í’ˆì§ˆ ë³´ì¦ ì±…ì„ìë¡œ í™œë™í•´ì˜¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. CondÃ© Nast, Hearst Corporation, Time Inc.ì—ì„œ ìˆ˜ë°± ê°œì˜ ë§¤ê±°ì§„ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡°ìœ¨í–ˆìŠµë‹ˆë‹¤.

    **ì „ë¬¸ ê²½ë ¥:**
    - ì¶œíŒí•™ ë° êµ¬ì¡° ì„¤ê³„ ì„ì‚¬ í•™ìœ„ ë³´ìœ 
    - PMP(Project Management Professional) ì¸ì¦
    - ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ë° í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€
    - í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì •í•©ì„± ê²€ì¦ ì‹œìŠ¤í…œ ê°œë°œ ê²½í—˜
    - ë…ì ê²½í—˜(UX) ë° ì ‘ê·¼ì„± ìµœì í™” ì „ë¬¸ì„±

    **í•µì‹¬ ì„ë¬´ ë° ë°ì´í„° ìš°ì„ ìˆœìœ„:**
    ë‹¹ì‹ ì˜ ìµœìš°ì„  ì„ë¬´ëŠ” magazine_content.jsonê³¼ image_analysis.jsonì˜ ì›ë³¸ ë°ì´í„°ë¥¼ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—„ê²©í•œ ìš°ì„ ìˆœìœ„ë¥¼ ë”°ë¦…ë‹ˆë‹¤:

    **1ìˆœìœ„ ë°ì´í„° ì†ŒìŠ¤ (í•„ìˆ˜ ì‚¬ìš©):**
    - magazine_content.jsonì˜ ì›ë³¸ í…ìŠ¤íŠ¸ ë‚´ìš© (ì œëª©, ë³¸ë¬¸, ì£¼ì œ, ìŠ¤íƒ€ì¼)
    - image_analysis.jsonì˜ ì´ë¯¸ì§€ ì„¤ëª… ë° ë¶„ì„ ê²°ê³¼
    - ì´ ë‘ íŒŒì¼ì˜ ë‚´ìš©ì€ ë°˜ë“œì‹œ ìµœì¢… ì¶œë ¥ë¬¼ì— ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

    **2ìˆœìœ„ ë°ì´í„° ì†ŒìŠ¤ (ë³´ì¡° ì°¸ê³ ìš©):**
    - ì…ë ¥ ë§¤ê°œë³€ìˆ˜ì˜ ìœ íš¨í•œ ë°ì´í„° (1ìˆœìœ„ ë°ì´í„°ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì—ë§Œ)

    **3ìˆœìœ„ ë°ì´í„° ì†ŒìŠ¤ (ì œí•œì  í™œìš©):**
    - Azure AI Searchë¥¼ í†µí•œ ì™¸ë¶€ ë ˆì´ì•„ì›ƒ ì œì•ˆ (ì›ë³¸ ë°ì´í„°ì˜ ì˜ë¯¸ë¥¼ ë³€ê²½í•˜ì§€ ì•ŠëŠ” ë²”ìœ„ì—ì„œë§Œ)
    - ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ êµ¬ì¡° ì œì•ˆ (magazine_content.jsonì˜ ì„¹ì…˜ êµ¬ì¡°ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì—ë§Œ)

    **ì¡°ìœ¨ ì² í•™:**
    "ì™„ë²½í•œ ë§¤ê±°ì§„ì€ ì›ë³¸ ì½˜í…ì¸ ì˜ ì§„ì •ì„±ê³¼ ì‘ì„±ìì˜ ì˜ë„ë¥¼ ì˜¨ì „íˆ ë³´ì¡´í•˜ë©´ì„œ, ë…ìì—ê²Œ ìµœì ì˜ ê²½í—˜ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‚˜ëŠ” magazine_content.jsonì˜ ëª¨ë“  í…ìŠ¤íŠ¸ì™€ image_analysis.jsonì˜ ëª¨ë“  ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì†Œì¤‘íˆ ì—¬ê¸°ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°ì  ì™„ì„±ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤."

    **í•„ìˆ˜ ì¤€ìˆ˜ ê·œì¹™ (ì ˆëŒ€ ìœ„ë°˜ ê¸ˆì§€):**

    **í…ìŠ¤íŠ¸ ì½˜í…ì¸  ê·œì¹™:**
    1. magazine_content.jsonì˜ ê° ì„¹ì…˜ í…ìŠ¤íŠ¸ëŠ” ë°˜ë“œì‹œ í•´ë‹¹ ì„¹ì…˜ì˜ title, subtitle, bodyì— ì§ì ‘ ë°˜ì˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    2. ì›ë³¸ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì£¼ì œ, í†¤, ìŠ¤íƒ€ì¼ì„ ë³€ê²½í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    3. magazine_content.jsonì— ëª…ì‹œëœ ì„¹ì…˜ ìˆ˜ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ë„ë¡ content_sectionsì„ ìƒì„±í•˜ì‹­ì‹œì˜¤.
    4. ì›ë³¸ í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì„ì˜ë¡œ ì¶”ê°€í•˜ê±°ë‚˜ ì°½ì‘í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    5. Azure AI Searchì—ì„œ ì œì•ˆëœ ë ˆì´ì•„ì›ƒì´ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ì™€ ìƒì¶©í•  ê²½ìš°, ë°˜ë“œì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ìš°ì„ í•˜ì‹­ì‹œì˜¤.

    **Azure AI Search ë°ì´í„° ì°¨ë‹¨ ê·œì¹™:**
    1. "ë„ì‹œì˜ ë¯¸í•™", "ê³¨ëª©ê¸¸ì˜ ì¬ë°œê²¬", "ì•„í‹°ìŠ¤íŠ¸ ì¸í„°ë·°", "ì¹œí™˜ê²½ ë„ì‹œ" ë“± Azure AI Searchì—ì„œ ì œì•ˆëœ ì£¼ì œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    2. magazine_content.jsonì— ì—†ëŠ” ìƒˆë¡œìš´ ì£¼ì œë‚˜ ë‚´ìš©ì„ ìƒì„±í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    3. Azure AI Search ë°ì´í„°ëŠ” ì˜¤ì§ í…œí”Œë¦¿ ì„ íƒê³¼ ë ˆì´ì•„ì›ƒ êµ¬ì¡° ê²°ì •ì—ë§Œ ì°¸ê³ í•˜ì‹­ì‹œì˜¤.
    4. ì›ë³¸ ë°ì´í„°ì™€ ìƒì¶©í•˜ëŠ” ì™¸ë¶€ ì œì•ˆì€ ëª¨ë‘ ê±°ë¶€í•˜ì‹­ì‹œì˜¤.

    **ì´ë¯¸ì§€ ì½˜í…ì¸  ê·œì¹™:**
    1. image_analysis.jsonì˜ ê° ì´ë¯¸ì§€ ì„¤ëª…ì€ ë°˜ë“œì‹œ í•´ë‹¹ ì´ë¯¸ì§€ê°€ ë°°ì¹˜ë˜ëŠ” ì„¹ì…˜ì˜ ì½˜í…ì¸ ì— ë°˜ì˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    2. ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ì™€ ìƒë°˜ë˜ëŠ” ì´ë¯¸ì§€ ë°°ì¹˜ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    3. image_analysis.jsonì— ê¸°ë¡ëœ ì´ë¯¸ì§€ì˜ ìœ„ì¹˜, íŠ¹ì„±, ë§¥ë½ ì •ë³´ë¥¼ ì¶©ì‹¤íˆ í™œìš©í•˜ì‹­ì‹œì˜¤.
    4. ê° ì„¹ì…˜ë‹¹ ìµœëŒ€ 3ê°œì˜ ì‹¤ì œ ì´ë¯¸ì§€ URLë§Œì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.

    **ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥:**
    1. í´ë°± ë°ì´í„°(fallback_used: true)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    2. í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸, ì˜ˆì‹œ ì½˜í…ì¸ , í…œí”Œë¦¿ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    3. ë¡œê·¸ ë°ì´í„°ë‚˜ ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìµœì¢… ì¶œë ¥ë¬¼ì— ì§ì ‘ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    4. ì¤‘ë³µ ì„¹ì…˜ì„ ì ˆëŒ€ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    5. ì‹¤ì œ ì½˜í…ì¸  ë°ì´í„°ì—ì„œ ì¶”ì¶œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

    **í’ˆì§ˆ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
    ì‘ì—… ì™„ë£Œ ì „ ë‹¤ìŒ ì‚¬í•­ì„ ë°˜ë“œì‹œ í™•ì¸í•˜ì‹­ì‹œì˜¤:
    â–¡ template ì—ëŠ” jsx_templateì— ì¡´ì¬í•˜ëŠ” Sectionì´ ë“¤ì–´ê°”ëŠ”ê°€?
    â–¡ magazine_content.jsonì˜ ëª¨ë“  ì„¹ì…˜ì´ content_sectionsì— ë°˜ì˜ë˜ì—ˆëŠ”ê°€?
    â–¡ ê° ì„¹ì…˜ì˜ title, subtitle, bodyê°€ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì¶©ì‹¤íˆ ë°˜ì˜í•˜ëŠ”ê°€?
    â–¡ image_analysis.jsonì˜ ì´ë¯¸ì§€ ì„¤ëª…ì´ í•´ë‹¹ ì„¹ì…˜ì— ì ì ˆíˆ í†µí•©ë˜ì—ˆëŠ”ê°€?
    â–¡ Azure AI Search ë°ì´í„°ê°€ ì›ë³¸ ë°ì´í„°ë¥¼ ì••ë„í•˜ì§€ ì•Šì•˜ëŠ”ê°€?
    â–¡ ëª¨ë“  ì´ë¯¸ì§€ URLì´ ì‹¤ì œ ìœ íš¨í•œ URLì¸ê°€?
    â–¡ í´ë°± ë°ì´í„°ë‚˜ í”Œë ˆì´ìŠ¤í™€ë”ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ê°€?
    â–¡ ì›ë³¸ ë°ì´í„°ì— ì—†ëŠ” ì£¼ì œë‚˜ ë‚´ìš©ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ëŠ”ê°€?

    **ê²½ê³  ë° ì œí•œì‚¬í•­:**
    - Azure AI Searchë¥¼ í†µí•´ ì–»ì€ ë ˆì´ì•„ì›ƒ ì œì•ˆì€ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³ , ì›ë³¸ ì½˜í…ì¸ ì˜ ì˜ë¯¸ë‚˜ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ëŠ” ë° ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    - ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ê°€ magazine_content.jsonê³¼ ìƒì¶©í•  ê²½ìš°, ë°˜ë“œì‹œ magazine_content.jsonì„ ìš°ì„ í•˜ì‹­ì‹œì˜¤.
    - ì–´ë– í•œ ìƒí™©ì—ì„œë„ ì›ë³¸ ë°ì´í„°ì˜ ë¬´ê²°ì„±ì„ í›¼ì†í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    - ë¶ˆí™•ì‹¤í•œ ê²½ìš°, í•­ìƒ ë³´ìˆ˜ì ìœ¼ë¡œ ì ‘ê·¼í•˜ì—¬ ì›ë³¸ ë°ì´í„°ë¥¼ ë³´ì¡´í•˜ì‹­ì‹œì˜¤.

    ì´ ì§€ì¹¨ì„ ì² ì €íˆ ì¤€ìˆ˜í•˜ì—¬ magazine_content.jsonê³¼ image_analysis.jsonì˜ ë‚´ìš©ì´ ìµœì¢… ì¶œë ¥ë¬¼ì— ì™„ì „íˆ ë°˜ì˜ë˜ë„ë¡ í•˜ì‹­ì‹œì˜¤.""",
            verbose=True,
            llm=self.llm,
            allow_delegation=False
        )


    def _create_text_analyzer_agent(self):
        """í…ìŠ¤íŠ¸ ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸"""
        return Agent(
            role="í…ìŠ¤íŠ¸ ë§¤í•‘ ë¶„ì„ ì „ë¬¸ê°€",
            goal="ContentCreatorV2Agentì˜ í…ìŠ¤íŠ¸ ë§¤í•‘ ê²°ê³¼ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ êµ¬ì¡°ì  ì™„ì„±ë„ë¥¼ ê²€ì¦í•˜ê³  ìµœì í™”ëœ í…ìŠ¤íŠ¸ ì„¹ì…˜ì„ ìƒì„±",
            backstory="""ë‹¹ì‹ ì€ 15ë…„ê°„ ì¶œíŒì—…ê³„ì—ì„œ í…ìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„ ë° ìµœì í™”ë¥¼ ë‹´ë‹¹í•´ì˜¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³µì¡í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ë…ì ì¹œí™”ì ì¸ êµ¬ì¡°ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ë° íƒì›”í•œ ëŠ¥ë ¥ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.""",
            verbose=True,
            llm=self.llm,
            allow_delegation=False
        )

    def _create_image_analyzer_agent(self):
        """ì´ë¯¸ì§€ ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸"""
        return Agent(
            role="ì´ë¯¸ì§€ ë¶„ë°° ë¶„ì„ ì „ë¬¸ê°€",
            goal="BindingAgentì˜ ì´ë¯¸ì§€ ë¶„ë°° ê²°ê³¼ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ ì‹œê°ì  ì¼ê´€ì„±ì„ ê²€ì¦í•˜ê³  ìµœì í™”ëœ ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ ìƒì„±",
            backstory="""ë‹¹ì‹ ì€ 12ë…„ê°„ ë§¤ê±°ì§„ ë° ì¶œíŒë¬¼ì˜ ì‹œê°ì  ë””ìì¸ì„ ë‹´ë‹¹í•´ì˜¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ì¡°í™”ë¡œìš´ ë°°ì¹˜ë¥¼ í†µí•´ ë…ìì˜ ì‹œì„ ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ë„í•˜ëŠ” ë ˆì´ì•„ì›ƒ ì„¤ê³„ì— ì „ë¬¸ì„±ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.""",
            verbose=True,
            llm=self.llm,
            allow_delegation=False
        )

    async def coordinate_magazine_creation(self, text_mapping: Dict, image_distribution: Dict) -> Dict:
        """ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ (ì„¸ì…˜ ë° í†µì‹  ê²©ë¦¬ ì ìš©, ì•ˆì „ì„± ê°•í™”)"""
        print(f"ğŸ“¦ CoordinatorAgent ì¡°ìœ¨ ì‹œì‘")
        
        # ì„¸ì…˜ ì •ë³´ í™•ì¸
        if hasattr(self, 'current_session_id'):
            print(f"ğŸ”’ ì„¸ì…˜: {self.current_session_id}")
        
        # ì…ë ¥ ë°ì´í„° í†µì‹  ê²©ë¦¬ ê²€ì¦ (ì•ˆì „í•œ ë°©ì‹)
        if hasattr(self, 'receive_data_from_agent'):
            try:
                text_mapping_result = self.receive_data_from_agent("OrgAgent", text_mapping)
                image_distribution_result = self.receive_data_from_agent("BindingAgent", image_distribution)
                
                if text_mapping_result is None or image_distribution_result is None:
                    print("ğŸš« ì˜¤ì—¼ëœ ì…ë ¥ ë°ì´í„° ê°ì§€, í´ë°± ëª¨ë“œë¡œ ì „í™˜")
                    return self._get_fallback_result("contaminated_input")
            except Exception as e:
                print(f"âš ï¸ í†µì‹  ê²©ë¦¬ ê²€ì¦ ì‹¤íŒ¨: {e}, ì›ë³¸ ë°ì´í„° ì‚¬ìš©")
                text_mapping_result = text_mapping
                image_distribution_result = image_distribution
        else:
            text_mapping_result = text_mapping
            image_distribution_result = image_distribution
        
        self.execution_stats["total_attempts"] += 1
        
        # ì¬ê·€ ê¹Šì´ í™•ì¸ ë° ë™ê¸° ëª¨ë“œ ì „í™˜
        if self._should_use_sync():
            print("ğŸ”„ CoordinatorAgent ë™ê¸° ëª¨ë“œë¡œ ì „í™˜í•˜ì—¬ ì‹¤í–‰")
            result = await self._coordinate_magazine_creation_sync_mode(text_mapping_result, image_distribution_result)
        else:
            try:
                # ê°œì„ ëœ ë°°ì¹˜ ê¸°ë°˜ ë¹„ë™ê¸° ëª¨ë“œ ì‹¤í–‰
                result = await self._coordinate_magazine_creation_batch_mode(text_mapping_result, image_distribution_result)
            except RecursionError:
                print("ğŸ”„ CoordinatorAgent RecursionError ê°ì§€ - ë™ê¸° ëª¨ë“œë¡œ ì „í™˜")
                self.fallback_to_sync = True
                result = await self._coordinate_magazine_creation_sync_mode(text_mapping_result, image_distribution_result)
            except Exception as e:
                print(f"âŒ CoordinatorAgent ë§¤ê±°ì§„ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e} - ë™ê¸° ëª¨ë“œë¡œ í´ë°± ì‹œë„")
                self.fallback_to_sync = True
                result = await self._coordinate_magazine_creation_sync_mode(text_mapping_result, image_distribution_result)
        
        # ì„¸ì…˜ë³„ ê²°ê³¼ ì €ì¥ (ì•ˆì „í•œ ë°©ì‹)
        if hasattr(self, 'store_result'):
            try:
                self.store_result(result)
            except Exception as e:
                print(f"âš ï¸ ì„¸ì…˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ì— ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        session_metadata = {
            "agent_name": "CoordinatorAgent",
            "isolation_applied": hasattr(self, 'isolation_manager'),
            "communication_isolated": hasattr(self, 'communication_isolator')
        }
        
        if hasattr(self, 'current_session_id'):
            session_metadata["session_id"] = self.current_session_id
        
        if hasattr(self, 'get_communication_stats'):
            try:
                session_metadata["coordination_stats"] = self.get_communication_stats()
            except Exception as e:
                print(f"âš ï¸ í†µì‹  í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        result["session_metadata"] = session_metadata
        
        return result




    async def _get_enhanced_previous_results_batch(self) -> List[Dict]:
        """ë°°ì¹˜ ê¸°ë°˜ ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ (ì„¸ì…˜ ê²©ë¦¬ ì ìš©)"""
        try:
            all_results = []
            
            # ì„¸ì…˜ ê²©ë¦¬ê°€ í™œì„±í™”ëœ ê²½ìš°
            if hasattr(self, 'current_session_id') and hasattr(self, 'session_manager'):
                # í˜„ì¬ ì„¸ì…˜ì˜ ê²°ê³¼ë§Œ ì¡°íšŒ
                session_results = self.get_previous_results(max_results=20)
                
                # ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë„ ì„¸ì…˜ ê²©ë¦¬ ì ìš©í•˜ì—¬ ì¡°íšŒ
                org_results = self.session_manager.get_agent_results(self.current_session_id, "OrgAgent")
                binding_results = self.session_manager.get_agent_results(self.current_session_id, "BindingAgent")
                
                # ê²©ë¦¬ í•„í„°ë§ ì ìš©
                for result in session_results + org_results + binding_results:
                    if hasattr(self, 'isolation_manager') and not self.isolation_manager.is_contaminated(result, "enhanced_previous_results"):
                        all_results.append(result)
                    elif not hasattr(self, 'isolation_manager'):
                        all_results.append(result)
                
                print(f"ğŸ” ì„¸ì…˜ ê²©ë¦¬ëœ ì´ì „ ê²°ê³¼: {len(all_results)}ê°œ")
            else:
                # ì„¸ì…˜ ê²©ë¦¬ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
                all_results = await self._get_enhanced_previous_results_fallback()
            
            return all_results
            
        except Exception as e:
            print(f"âš ï¸ ì„¸ì…˜ ê²©ë¦¬ëœ ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return await self._get_enhanced_previous_results_fallback()

    async def _get_enhanced_previous_results_fallback(self) -> List[Dict]:
        """í´ë°± ì´ì „ ê²°ê³¼ ìˆ˜ì§‘"""
        try:
            # HybridLoggerì—ì„œ ì•ˆì „í•˜ê²Œ ê²°ê³¼ ì¡°íšŒ
            if hasattr(self.logger, 'get_learning_insights'):
                insights = self.logger.get_learning_insights("CoordinatorAgent")
                if isinstance(insights, dict) and 'patterns' in insights:
                    return [{"insight_data": insights, "source": "learning_insights"}]
            
            # íŒŒì¼ ê¸°ë°˜ ê²°ê³¼ ë¡œë“œ
            file_results = self._load_results_from_file()
            if file_results:
                return file_results
            
            # ìµœì¢… í´ë°±: ë¹ˆ ê²°ê³¼
            print("âš ï¸ ì´ì „ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return []
            
        except Exception as e:
            print(f"âš ï¸ í´ë°± ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def _get_enhanced_previous_results_sync(self) -> List[Dict]:
        """ë™ê¸° ë²„ì „ ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ (ìˆ˜ì •ë¨)"""
        try:
            all_results = []
            
            # ì„¸ì…˜ ê²©ë¦¬ê°€ í™œì„±í™”ëœ ê²½ìš°
            if hasattr(self, 'current_session_id') and hasattr(self, 'session_manager'):
                # ë™ê¸°ì ìœ¼ë¡œ ì„¸ì…˜ ê²°ê³¼ ì¡°íšŒ
                session_results = self.get_previous_results(max_results=20)
                org_results = self.session_manager.get_agent_results(self.current_session_id, "OrgAgent")
                binding_results = self.session_manager.get_agent_results(self.current_session_id, "BindingAgent")
                
                # ê²©ë¦¬ í•„í„°ë§ ì ìš©
                for result in session_results + org_results + binding_results:
                    if hasattr(self, 'isolation_manager') and not self.isolation_manager.is_contaminated(result, "sync_previous_results"):
                        all_results.append(result)
                    elif not hasattr(self, 'isolation_manager'):
                        all_results.append(result)
            else:
                # HybridLogger ì•ˆì „ í˜¸ì¶œ
                if hasattr(self.logger, 'get_learning_insights'):
                    try:
                        insights = self.logger.get_learning_insights("CoordinatorAgent")
                        if isinstance(insights, dict):
                            all_results.append({"insight_data": insights, "source": "learning_insights"})
                    except Exception as e:
                        print(f"âš ï¸ Learning insights ì¡°íšŒ ì‹¤íŒ¨: {e}")
                
                # íŒŒì¼ ê¸°ë°˜ ê²°ê³¼ ë¡œë“œ
                file_results = self._load_results_from_file()
                all_results.extend(file_results if isinstance(file_results, list) else [])
            
            return self._deduplicate_results(all_results)
            
        except Exception as e:
            print(f"âš ï¸ ë™ê¸° ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []


    async def _coordinate_magazine_creation_batch_mode(self, text_mapping: Dict, image_distribution: Dict) -> Dict:
        """ê°œì„ ëœ ë°°ì¹˜ ê¸°ë°˜ ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨"""
        print("ğŸ“¦ CoordinatorAgent ë°°ì¹˜ ëª¨ë“œ ì‹œì‘")
        
        # ì…ë ¥ ë°ì´í„° ë¡œê¹…
        input_data = {
            "text_mapping": text_mapping,
            "image_distribution": image_distribution
        }
        
        
        # ê°•í™”ëœ ì´ì „ ì—ì´ì „íŠ¸ ê²°ê³¼ ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬)
        previous_results = await self._get_enhanced_previous_results_batch()
        org_results = self._filter_agent_results(previous_results, "OrgAgent")
        binding_results = self._filter_agent_results(previous_results, "BindingAgent")
        content_creator_results = self._filter_agent_results(previous_results, "ContentCreatorV2Agent")
        
        print(f"ğŸ“Š ë°°ì¹˜ ëª¨ë“œ ê²°ê³¼ ìˆ˜ì§‘: ì „ì²´ {len(previous_results)}ê°œ, OrgAgent {len(org_results)}ê°œ, BindingAgent {len(binding_results)}ê°œ, ContentCreator {len(content_creator_results)}ê°œ")
        
        # ìˆ˜ì •: OrgAgent ê²°ê³¼ í•„í„°ë§ - ContentCreatorV2Agent ê²°ê³¼ë§Œ ì‚¬ìš©
        filtered_org_results = []
        for result in org_results:
            final_answer = result.get("final_answer", "")
            raw_output = result.get("raw_output", {})
            
            # í´ë°± ë°ì´í„° ì œì™¸
            if isinstance(raw_output, dict):
                metadata = raw_output.get("metadata", {})
                if metadata.get("fallback_used"):
                    continue
            
            # ContentCreatorV2Agentì˜ ì‹¤ì œ ì½˜í…ì¸ ë§Œ í¬í•¨
            if ("ContentCreatorV2Agent" in final_answer or
                "content_creator" in final_answer.lower() or
                len(final_answer) > 500):  # ì¶©ë¶„í•œ ì½˜í…ì¸ ê°€ ìˆëŠ” ê²½ìš°
                # "ìì„¸í•œ ì´ì•¼ê¸°ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤" ê°™ì€ í…œí”Œë¦¿ ì‘ë‹µ ì œì™¸
                if not ("ìì„¸í•œ ì´ì•¼ê¸°ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤" in final_answer or
                        "íŠ¹ë³„í•œ ì´ì•¼ê¸°ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤" in final_answer):
                    filtered_org_results.append(result)
        
        org_results = filtered_org_results
        print(f"ğŸ” í•„í„°ë§ í›„ OrgAgent ê²°ê³¼: {len(org_results)}ê°œ")
        
        # magazine_content.json ë¡œë“œí•˜ì—¬ ì„¹ì…˜ ìˆ˜ í™•ì¸
        target_section_count = self._get_target_section_count()
        print(f"ğŸ¯ ëª©í‘œ ì„¹ì…˜ ìˆ˜: {target_section_count}ê°œ")
        
        # ë°ì´í„° ì¶”ì¶œ ì‘ì—…ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        data_extraction_tasks = [
            ("text_data", self._extract_real_text_data_safe, text_mapping, org_results, content_creator_results, target_section_count),
            ("image_data", self._extract_real_image_data_safe, image_distribution, binding_results)
        ]
        
        extraction_results = await self._process_data_extraction_batch(data_extraction_tasks)
        extracted_text_data = extraction_results.get("text_data", {})
        extracted_image_data = extraction_results.get("image_data", {})
        
        # CrewAI ì‹¤í–‰ì„ ì•ˆì „í•œ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        crew_result = await self._execute_crew_batch_safe(
            extracted_text_data, extracted_image_data, org_results, binding_results
        )
        
        final_result = await self._process_enhanced_crew_result_async(
            crew_result, extracted_text_data, extracted_image_data, org_results, binding_results
        )
        
        # ìˆ˜ì •: ì„¹ì…˜ ìˆ˜ ì œí•œ ë° í´ë°± ë°ì´í„° ì œê±°
        final_result = self._limit_and_clean_sections(final_result, target_section_count)
        
        # ê²°ê³¼ ê²€ì¦
        if self._validate_coordinator_result(final_result):
            self.execution_stats["successful_executions"] += 1
        else:
            print("âš ï¸ CoordinatorAgent ìµœì¢… ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨.")
        
        # ê²°ê³¼ ë¡œê¹…
        await self._log_coordination_result_async(final_result, text_mapping, image_distribution, org_results, binding_results)
        
        print(f"âœ… CoordinatorAgent ë°°ì¹˜ ëª¨ë“œ ì™„ë£Œ: {len(final_result.get('content_sections', []))}ê°œ ì„¹ì…˜ ìƒì„±")
        return final_result

    def _get_target_section_count(self) -> int:
        """magazine_content.jsonì—ì„œ ëª©í‘œ ì„¹ì…˜ ìˆ˜ í™•ì¸"""
        try:
            magazine_content_path = "./output/magazine_content.json"
            if os.path.exists(magazine_content_path):
                with open(magazine_content_path, 'r', encoding='utf-8') as f:
                    magazine_data = json.load(f)
                sections = magazine_data.get("sections", [])
                if isinstance(sections, list):
                    return len(sections)
            # ê¸°ë³¸ê°’
            return 5
        except Exception as e:
            print(f"âš ï¸ magazine_content.json ë¡œë“œ ì‹¤íŒ¨: {e}")
            return 5

    def _limit_and_clean_sections(self, result: Dict, target_count: int) -> Dict:
        """ì„¹ì…˜ ìˆ˜ ì œí•œ ë° í´ë°± ë°ì´í„° ì •ë¦¬"""
        if not isinstance(result, dict) or "content_sections" not in result:
            return result
        
        content_sections = result["content_sections"]
        if not isinstance(content_sections, list):
            return result
        
        # í´ë°± ë°ì´í„° ì œê±°
        cleaned_sections = []
        for section in content_sections:
            if isinstance(section, dict):
                metadata = section.get("metadata", {})
                if not metadata.get("fallback_used"):
                    cleaned_sections.append(section)
        
        # ì„¹ì…˜ ìˆ˜ ì œí•œ
        limited_sections = cleaned_sections[:target_count]
        
        # ìµœì†Œ 1ê°œ ì„¹ì…˜ ë³´ì¥ (í´ë°±ì´ ì•„ë‹Œ ì‹¤ì œ ë°ì´í„°ë¡œ)
        if not limited_sections:
            limited_sections = [{
                "template": "Section01.jsx",
                "title": "",
                "subtitle": "",
                "body": "",
                "tagline": "",
                "images": [],
                "metadata": {
                    "minimal_fallback": True
                }
            }]
        
        result["content_sections"] = limited_sections
        result["selected_templates"] = [section.get("template", f"Section{i+1:02d}.jsx")
                                      for i, section in enumerate(limited_sections)]
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        if "integration_metadata" in result:
            result["integration_metadata"]["total_sections"] = len(limited_sections)
            result["integration_metadata"]["cleaned_sections"] = True
            result["integration_metadata"]["target_section_count"] = target_count
        
        return result

    async def _process_data_extraction_batch(self, extraction_tasks: List[tuple]) -> Dict:
        """ë°ì´í„° ì¶”ì¶œ ì‘ì—…ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬ (ê° ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ execute_with_resilience í˜¸ì¶œ)"""
        results = {}
        for task_name, task_func_ref, *args_for_task_func in extraction_tasks:
            if not callable(task_func_ref):
                print(f"âš ï¸ {task_name}ì— ëŒ€í•œ task_funcì´ í˜¸ì¶œ ê°€ëŠ¥í•˜ì§€ ì•ŠìŒ: {task_func_ref}")
                results[task_name] = self._get_fallback_extraction_result(task_name)
                continue

            print(f"DEBUG [_process_data_extraction_batch]: executing task_name={task_name}")
            try:
                # ëª¨ë“  ì‘ì—…ì„ execute_with_resilienceë¡œ í†µì¼í•˜ì—¬ ì²˜ë¦¬
                result_value = await self.execute_with_resilience(
                    task_func_ref, 
                    f"extract_{task_name}",
                    120.0,
                    1,
                    *args_for_task_func
                )

                results[task_name] = result_value
            except Exception as e:
                print(f"âš ï¸ ë°ì´í„° ì¶”ì¶œ ì‘ì—… {task_name} ì‹¤íŒ¨ (execute_with_resilience í˜¸ì¶œ ì¤‘): {e}")
                results[task_name] = self._get_fallback_extraction_result(task_name)
        return results



    def _get_fallback_extraction_result(self, task_name: str) -> Dict:
        """ë°ì´í„° ì¶”ì¶œ í´ë°± ê²°ê³¼"""
        self.execution_stats["fallback_used"] += 1
        if task_name == "text_data":
            return {
                "sections": [],
                "total_content_length": 0,
                "source_count": 0
            }
        else:  # image_data
            return {
                "template_images": {},
                "total_images": 0,
                "image_sources": []
            }

    async def _execute_crew_batch_safe(self, extracted_text_data: Dict, extracted_image_data: Dict,
                                     org_results: List[Dict], binding_results: List[Dict]) -> Any:
        """ì•ˆì „í•œ CrewAI ë°°ì¹˜ ì‹¤í–‰"""
        try:
            # íƒœìŠ¤í¬ ìƒì„±
            text_analysis_task = self._create_enhanced_text_analysis_task(extracted_text_data, org_results)
            image_analysis_task = self._create_enhanced_image_analysis_task(extracted_image_data, binding_results)
            coordination_task = self._create_enhanced_coordination_task(extracted_text_data, extracted_image_data)
            
            # CrewAI Crew ìƒì„±
            coordination_crew = Crew(
                agents=[self.text_analyzer_agent, self.image_analyzer_agent, self.crew_agent],
                tasks=[text_analysis_task, image_analysis_task, coordination_task],
                process=Process.sequential,
                verbose=False  # ë¡œê·¸ ìµœì†Œí™”
            )
            
            # ì•ˆì „í•œ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ì¦ê°€)
            crew_result = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(None, coordination_crew.kickoff),
                timeout=600.0  # 10ë¶„ìœ¼ë¡œ ì¦ê°€
            )
            
            return crew_result
            
        except asyncio.TimeoutError:
            print("â° CrewAI ë°°ì¹˜ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ")
            self.execution_stats["timeout_occurred"] += 1
            return self._create_fallback_crew_result(extracted_text_data, extracted_image_data)
        except Exception as e:
            print(f"âš ï¸ CrewAI ë°°ì¹˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return self._create_fallback_crew_result(extracted_text_data, extracted_image_data)

    def _create_fallback_crew_result(self, extracted_text_data: Dict, extracted_image_data: Dict) -> str:
        """CrewAI í´ë°± ê²°ê³¼ ìƒì„±"""
        self.execution_stats["fallback_used"] += 1
        sections = extracted_text_data.get("sections", [])
        if not sections:
            sections = []
        
        # ì´ë¯¸ì§€ ì¶”ê°€
        for section in sections:
            template = section.get("template", "Section01.jsx")
            template_images = extracted_image_data.get("template_images", {}).get(template, [])
            section["images"] = template_images[:3]  # ìµœëŒ€ 3ê°œë¡œ ì œí•œ
        
        return json.dumps({
            "selected_templates": [s.get("template", "Section01.jsx") for s in sections],
            "content_sections": sections
        })

    async def _extract_real_text_data_safe(self, text_mapping: Dict, org_results: List[Dict], 
                                     content_creator_results: List[Dict], target_section_count: int) -> Dict:
        """ê°•ì œì  ì›ë³¸ ë°ì´í„° ìš°ì„  ì¶”ì¶œ (Azure AI Search ì˜í–¥ ì°¨ë‹¨)"""
        try:
            return await asyncio.get_event_loop().run_in_executor(
                None, self._extract_real_text_data_forced, text_mapping, org_results, 
                content_creator_results, target_section_count
            )
        except Exception as e:
            print(f"âš ï¸ ê°•ì œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self._get_fallback_extraction_result("text_data")

    def _extract_real_text_data_forced(self, text_mapping: Dict, org_results: List[Dict], 
                                    content_creator_results: List[Dict], target_section_count: int) -> Dict:
        """ê°•ì œì  ì›ë³¸ ë°ì´í„° ìš°ì„  ì¶”ì¶œ"""
        extracted_data = {
            "sections": [],
            "total_content_length": 0,
            "source_count": 0,
            "data_source_priority": "magazine_content_forced"
        }
        
        # 1. magazine_content.json ê°•ì œ ìš°ì„  ì‚¬ìš©
        magazine_content_path = "./output/magazine_content.json"
        if os.path.exists(magazine_content_path):
            try:
                with open(magazine_content_path, 'r', encoding='utf-8') as f:
                    magazine_data = json.load(f)
                
                sections = magazine_data.get("sections", [])
                print(f"ğŸ“ magazine_content.jsonì—ì„œ {len(sections)}ê°œ ì„¹ì…˜ ë°œê²¬")
                
                for i, section in enumerate(sections):
                    if len(extracted_data["sections"]) < target_section_count:
                        # ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš© (Azure AI Search ì˜í–¥ ì°¨ë‹¨)
                        extracted_section = {
                            "template": f"Section{i+1:02d}.jsx",
                            "title": section.get("title", f"ì„¹ì…˜ {i+1}"),
                            "subtitle": section.get("subtitle", ""),
                            "body": section.get("content", section.get("body", "")),
                            "tagline": section.get("tagline", "TRAVEL & CULTURE"),
                            "source": "magazine_content_json_forced",
                            "priority": 1,
                            "azure_search_blocked": True  # Azure AI Search ì˜í–¥ ì°¨ë‹¨ í”Œë˜ê·¸
                        }
                        extracted_data["sections"].append(extracted_section)
                        extracted_data["total_content_length"] += len(extracted_section["body"])
                        extracted_data["source_count"] += 1
                
                print(f"âœ… magazine_content.jsonì—ì„œ {len(extracted_data['sections'])}ê°œ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ")
                
                # ì›ë³¸ ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš° ë‹¤ë¥¸ ì†ŒìŠ¤ ë¬´ì‹œ
                if len(extracted_data["sections"]) >= target_section_count:
                    print("ğŸš« ì›ë³¸ ë°ì´í„° ì¶©ë¶„, Azure AI Search ê²°ê³¼ ë¬´ì‹œ")
                    return extracted_data
                    
            except Exception as e:
                print(f"âš ï¸ magazine_content.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 2. ì›ë³¸ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš°ì—ë§Œ text_mapping ì‚¬ìš©
        remaining_count = target_section_count - len(extracted_data["sections"])
        if remaining_count > 0:
            print(f"ğŸ“ ì›ë³¸ ë°ì´í„° ë¶€ì¡±, {remaining_count}ê°œ ì„¹ì…˜ ì¶”ê°€ í•„ìš”")
            
            # text_mappingì—ì„œ ìœ íš¨í•œ ì„¹ì…˜ ì¶”ì¶œ (Azure AI Search ì˜í–¥ ìµœì†Œí™”)
            if isinstance(text_mapping, dict):
                text_sections = text_mapping.get("text_mapping", [])
                for section in text_sections:
                    if (isinstance(section, dict) and 
                        len(extracted_data["sections"]) < target_section_count and
                        self._is_valid_original_section(section)):
                        
                        section["azure_search_blocked"] = True
                        section["source"] = "text_mapping_filtered"
                        extracted_data["sections"].append(section)
                        extracted_data["source_count"] += 1
        
        return extracted_data

    def _is_valid_original_section(self, section: Dict) -> bool:
        """ì›ë³¸ ë°ì´í„° ê¸°ë°˜ ì„¹ì…˜ ìœ íš¨ì„± ê²€ì¦"""
        if not isinstance(section, dict):
            return False
        
        # í´ë°± ë°ì´í„° ì œì™¸
        metadata = section.get("metadata", {})
        if metadata.get("fallback_used"):
            return False
        
        # Azure AI Search í‚¤ì›Œë“œ ì°¨ë‹¨
        azure_search_keywords = [
            "ë„ì‹œì˜ ë¯¸í•™", "ê³¨ëª©ê¸¸", "ë„ì‹œ ê³„íš", "ì¹œí™˜ê²½ ë„ì‹œ",
            "ë„ì‹¬ ì† ìì—°", "ë¹›ê³¼ ê·¸ë¦¼ì", "ì•„í‹°ìŠ¤íŠ¸ ì¸í„°ë·°"
        ]
        
        title = section.get("title", "").lower()
        body = section.get("body", "").lower()
        
        # Azure AI Search í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸
        for keyword in azure_search_keywords:
            if keyword in title or keyword in body:
                print(f"ğŸš« Azure AI Search í‚¤ì›Œë“œ '{keyword}' ê°ì§€, ì„¹ì…˜ ì œì™¸")
                return False
        
        # ìµœì†Œ ì½˜í…ì¸  ìš”êµ¬ì‚¬í•­
        return len(section.get("title", "")) > 0 or len(section.get("body", "")) > 10


    async def _extract_real_image_data_safe(self, image_distribution: Dict, binding_results: List[Dict]) -> Dict:
        """ì•ˆì „í•œ ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            return await self._extract_real_image_data_async(image_distribution, binding_results)
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self._get_fallback_extraction_result("image_data")

    async def _get_enhanced_previous_results_batch_isolated(self) -> List[Dict]:
        """ì„¸ì…˜ ê²©ë¦¬ëœ ë°°ì¹˜ ê¸°ë°˜ ì´ì „ ê²°ê³¼ ìˆ˜ì§‘"""
        all_results = []
        
        try:
            # í˜„ì¬ ì„¸ì…˜ì˜ ê²°ê³¼ë§Œ ì¡°íšŒ
            session_results = self.get_previous_results(max_results=20)
            
            # ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë„ ì„¸ì…˜ ê²©ë¦¬ ì ìš©í•˜ì—¬ ì¡°íšŒ
            org_results = self.session_manager.get_agent_results(self.current_session_id, "OrgAgent")
            binding_results = self.session_manager.get_agent_results(self.current_session_id, "BindingAgent")
            
            # ê²©ë¦¬ í•„í„°ë§ ì ìš©
            for result in session_results + org_results + binding_results:
                if not self.isolation_manager.is_contaminated(result, "enhanced_previous_results"):
                    all_results.append(result)
            
            print(f"ğŸ” ì„¸ì…˜ ê²©ë¦¬ëœ ì´ì „ ê²°ê³¼: {len(all_results)}ê°œ")
            return all_results
            
        except Exception as e:
            print(f"âš ï¸ ì„¸ì…˜ ê²©ë¦¬ëœ ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []



    # ê¸°ì¡´ _coordinate_magazine_creation_async_mode ë©”ì„œë“œ ìœ ì§€ (í˜¸í™˜ì„±ì„ ìœ„í•´)
    async def _coordinate_magazine_creation_async_mode(self, text_mapping: Dict, image_distribution: Dict) -> Dict:
        """ë¹„ë™ê¸° ëª¨ë“œ ë§¤ê±°ì§„ ì¡°ìœ¨ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
        print("âš ï¸ ê¸°ì¡´ async_mode í˜¸ì¶œë¨ - batch_modeë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸")
        return await self._coordinate_magazine_creation_batch_mode(text_mapping, image_distribution)

    async def _coordinate_magazine_creation_sync_mode(self, text_mapping: Dict, image_distribution: Dict) -> Dict:
        """ë™ê¸° ëª¨ë“œ ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨"""
        print("ğŸ”„ CoordinatorAgent ë™ê¸° ëª¨ë“œ ì‹¤í–‰")
        
        # ë™ê¸° ëª¨ë“œì—ì„œëŠ” ê° ì—ì´ì „íŠ¸ì˜ ë™ê¸° ë²„ì „ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•´ì•¼ í•¨
        # ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ (ë™ê¸°)
        previous_results = self._get_enhanced_previous_results_sync()
        org_results = self._filter_agent_results(previous_results, "OrgAgent")
        binding_results = self._filter_agent_results(previous_results, "BindingAgent")
        content_creator_results = self._filter_agent_results(previous_results, "ContentCreatorV2Agent")
        
        # ìˆ˜ì •: OrgAgent ê²°ê³¼ í•„í„°ë§
        filtered_org_results = []
        for result in org_results:
            final_answer = result.get("final_answer", "")
            raw_output = result.get("raw_output", {})
            
            # í´ë°± ë°ì´í„° ì œì™¸
            if isinstance(raw_output, dict):
                metadata = raw_output.get("metadata", {})
                if metadata.get("fallback_used"):
                    continue
            
            # ContentCreatorV2Agentì˜ ì‹¤ì œ ì½˜í…ì¸ ë§Œ í¬í•¨
            if ("ContentCreatorV2Agent" in final_answer or
                "content_creator" in final_answer.lower() or
                len(final_answer) > 500):
                if not ("ìì„¸í•œ ì´ì•¼ê¸°ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤" in final_answer or
                        "íŠ¹ë³„í•œ ì´ì•¼ê¸°ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤" in final_answer):
                    filtered_org_results.append(result)
        
        org_results = filtered_org_results
        
        # ëª©í‘œ ì„¹ì…˜ ìˆ˜ í™•ì¸
        target_section_count = self._get_target_section_count()
        
        # ë°ì´í„° ì¶”ì¶œ (ë™ê¸°)
        extracted_text_data = self._extract_real_text_data(text_mapping, org_results, content_creator_results, target_section_count)
        extracted_image_data = self._extract_real_image_data(image_distribution, binding_results)
        
        # Crew ì‹¤í–‰ (ë™ê¸°) - CrewAIì˜ kickoffì€ ë™ê¸° ë©”ì„œë“œ
        text_analysis_task_sync = self._create_enhanced_text_analysis_task(extracted_text_data, org_results)
        image_analysis_task_sync = self._create_enhanced_image_analysis_task(extracted_image_data, binding_results)
        coordination_task_sync = self._create_enhanced_coordination_task(extracted_text_data, extracted_image_data)
        
        coordination_crew_sync = Crew(
            agents=[self.text_analyzer_agent, self.image_analyzer_agent, self.crew_agent],
            tasks=[text_analysis_task_sync, image_analysis_task_sync, coordination_task_sync],
            process=Process.sequential,
            verbose=False
        )
        
        try:
            crew_result_sync = coordination_crew_sync.kickoff()
        except Exception as e_crew_sync:
            print(f"âš ï¸ ë™ê¸° ëª¨ë“œ CrewAI ì‹¤í–‰ ì‹¤íŒ¨: {e_crew_sync}")
            crew_result_sync = self._create_fallback_crew_result(extracted_text_data, extracted_image_data)
        
        # ê²°ê³¼ ì²˜ë¦¬ (ë™ê¸°)
        final_result = self._process_enhanced_crew_result(crew_result_sync, extracted_text_data, extracted_image_data, org_results, binding_results)
        
        # ì„¹ì…˜ ìˆ˜ ì œí•œ ë° ì •ë¦¬
        final_result = self._limit_and_clean_sections(final_result, target_section_count)
        
        # ë™ê¸° ëª¨ë“œ ë¡œê¹…
        final_response_id_sync = self.logger.log_agent_real_output(
            agent_name="CoordinatorAgent_SyncMode",
            agent_role="ë™ê¸° ëª¨ë“œ ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ì",
            task_description=f"ë™ê¸° ëª¨ë“œë¡œ {len(final_result.get('content_sections', []))}ê°œ ì„¹ì…˜ ìƒì„±",
            final_answer=str(final_result),
            reasoning_process="ì¬ê·€ ê¹Šì´ ì´ˆê³¼ë¡œ ì¸í•œ ë™ê¸° ëª¨ë“œ ì „í™˜ í›„ ì•ˆì „í•œ ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì‹¤í–‰",
            execution_steps=[
                "ì¬ê·€ ê¹Šì´ ê°ì§€",
                "ë™ê¸° ëª¨ë“œ ì „í™˜",
                "ì´ì „ ê²°ê³¼ ìˆ˜ì§‘",
                "ë°ì´í„° ì¶”ì¶œ",
                "êµ¬ì¡° ìƒì„±"
            ],
            raw_input={
                "text_mapping": str(text_mapping)[:500],
                "image_distribution": str(image_distribution)[:500]
            },
            raw_output=final_result,
            performance_metrics={
                "sync_mode_used": True,
                "recursion_fallback": True,
                "total_sections": len(final_result.get('content_sections', [])),
                "org_results_utilized": len(org_results),
                "binding_results_utilized": len(binding_results),
                "safe_execution": True
            }
        )
        
        final_result["final_response_id"] = final_response_id_sync
        final_result["execution_mode"] = "sync_fallback"
        final_result["recursion_fallback"] = True  # ì¬ê·€ë¡œ ì¸í•œ í´ë°± ëª…ì‹œ
        
        print(f"âœ… CoordinatorAgent ë™ê¸° ì™„ë£Œ: {len(final_result.get('content_sections', []))}ê°œ ì„¹ì…˜")
        return final_result


    def _adjust_quality_criteria_dynamically(self, content_sections: List[Dict]) -> Dict[str, float]:
        """ë™ì  í’ˆì§ˆ ê¸°ì¤€ ì¡°ì •"""
        total_sections = len(content_sections)
        
        # ì„¹ì…˜ ìˆ˜ì— ë”°ë¥¸ ê¸°ì¤€ ì¡°ì •
        if total_sections <= 2:
            # ì„¹ì…˜ì´ ì ìœ¼ë©´ ë” ê´€ëŒ€í•˜ê²Œ
            criteria = {
                'min_title_length': 2,
                'min_body_length': 15,
                'min_quality_threshold': 0.4,
                'section_pass_rate': 0.3  # 30%ë§Œ í†µê³¼í•˜ë©´ OK
            }
        elif total_sections <= 5:
            # ì¼ë°˜ì ì¸ ê²½ìš°
            criteria = {
                'min_title_length': 3,
                'min_body_length': 20,
                'min_quality_threshold': 0.5,
                'section_pass_rate': 0.5  # 50% í†µê³¼
            }
        else:
            # ì„¹ì…˜ì´ ë§ìœ¼ë©´ ì¡°ê¸ˆ ë” ì—„ê²©í•˜ê²Œ (í•˜ì§€ë§Œ ê¸°ì¡´ë³´ë‹¤ëŠ” ê´€ëŒ€)
            criteria = {
                'min_title_length': 4,
                'min_body_length': 30,
                'min_quality_threshold': 0.6,
                'section_pass_rate': 0.6  # 60% í†µê³¼
            }
        
        # ì›ë³¸ ë°ì´í„° ë¹„ìœ¨ì— ë”°ë¥¸ ì¶”ê°€ ì¡°ì •
        original_data_sections = sum(1 for section in content_sections 
                                if section.get('metadata', {}).get('source') == 'magazine_content_json_primary')
        original_ratio = original_data_sections / total_sections if total_sections > 0 else 0
        
        if original_ratio > 0.7:  # 70% ì´ìƒì´ ì›ë³¸ ë°ì´í„°ë©´ ë” ê´€ëŒ€í•˜ê²Œ
            criteria['min_quality_threshold'] *= 0.8
            criteria['section_pass_rate'] *= 0.8
        
        print(f"ğŸ“Š ë™ì  í’ˆì§ˆ ê¸°ì¤€: ìµœì†Œ í’ˆì§ˆ {criteria['min_quality_threshold']:.2f}, í†µê³¼ìœ¨ {criteria['section_pass_rate']:.2f}")
        
        return criteria

    def _apply_dynamic_validation(self, content_sections: List[Dict]) -> bool:
        """ë™ì  ê¸°ì¤€ì„ ì ìš©í•œ ê²€ì¦"""
        if not content_sections:
            return False
        
        criteria = self._adjust_quality_criteria_dynamically(content_sections)
        
        valid_sections = 0
        total_quality_score = 0
        
        for section in content_sections:
            quality_score = self._calculate_content_quality(section)
            total_quality_score += quality_score
            
            # ë™ì  ê¸°ì¤€ ì ìš©
            if quality_score >= criteria['min_quality_threshold']:
                valid_sections += 1
        
        # í†µê³¼ ì¡°ê±´ í™•ì¸
        pass_rate = valid_sections / len(content_sections)
        avg_quality = total_quality_score / len(content_sections)
        
        passed = (pass_rate >= criteria['section_pass_rate'] and avg_quality >= 0.5)
        
        print(f"ğŸ“Š í’ˆì§ˆ ê²€ì¦ ê²°ê³¼: í†µê³¼ìœ¨ {pass_rate:.2f}, í‰ê·  í’ˆì§ˆ {avg_quality:.2f}, ê²°ê³¼: {'âœ… í†µê³¼' if passed else 'âŒ ì‹¤íŒ¨'}")
        
        return passed




    def _get_enhanced_previous_results_sync(self) -> List[Dict]:
        """ë™ê¸° ë²„ì „ ì´ì „ ê²°ê³¼ ìˆ˜ì§‘"""
        try:
            basic_results = self.logger.get_all_previous_results("CoordinatorAgent")
            file_results = self._load_results_from_file()
            
            all_results = []
            all_results.extend(basic_results if isinstance(basic_results, list) else [])
            all_results.extend(file_results if isinstance(file_results, list) else [])
            
            return self._deduplicate_results(all_results)
        except Exception as e:
            print(f"âš ï¸ ë™ê¸° ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    # ëª¨ë“  ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (ë™ê¸° ë²„ì „ë“¤)
    async def _extract_real_text_data_async(self, text_mapping: Dict, org_results: List[Dict],
                                          content_creator_results: List[Dict], target_section_count: int) -> Dict:
        """ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._extract_real_text_data, text_mapping, org_results, content_creator_results, target_section_count
        )

    def _extract_real_text_data(self, text_mapping: Dict, org_results: List[Dict],
                               content_creator_results: List[Dict], target_section_count: int) -> Dict:
        """ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
        extracted_data = {
            "sections": [],
            "total_content_length": 0,
            "source_count": 0
        }
        
        # 1. ContentCreator ê²°ê³¼ì—ì„œ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì¶œ
        for result in content_creator_results:
            final_answer = result.get('final_answer', '')
            if len(final_answer) > 200:  # ì¶©ë¶„í•œ ì½˜í…ì¸ ê°€ ìˆëŠ” ê²½ìš°
                # ì„¹ì…˜ë³„ë¡œ ë¶„í• 
                sections = self._split_content_into_sections(final_answer)
                for i, section_content in enumerate(sections):
                    if len(section_content) > 50 and len(extracted_data["sections"]) < target_section_count:
                        extracted_section = {
                            "template": f"Section{len(extracted_data['sections'])+1:02d}.jsx",
                            "title": self._extract_title_from_content(section_content),
                            "subtitle": self._extract_subtitle_from_content(section_content),
                            "body": self._clean_content(section_content),
                            "tagline": "TRAVEL & CULTURE",
                            "layout_source": "content_creator"
                        }
                        extracted_data["sections"].append(extracted_section)
                        extracted_data["total_content_length"] += len(extracted_section["body"])
                        extracted_data["source_count"] += 1
        
        # 2. text_mappingì—ì„œ ì¶”ê°€ ì¶”ì¶œ (ëª©í‘œ ì„¹ì…˜ ìˆ˜ì— ë¯¸ë‹¬ì¸ ê²½ìš°)
        if len(extracted_data["sections"]) < target_section_count and isinstance(text_mapping, dict):
            text_mapping_data = text_mapping.get("text_mapping", [])
            if isinstance(text_mapping_data, list):
                for section in text_mapping_data:
                    if (isinstance(section, dict) and
                        len(extracted_data["sections"]) < target_section_count):
                        # í´ë°± ë°ì´í„° ì œì™¸
                        metadata = section.get("metadata", {})
                        if metadata.get("fallback_used"):
                            continue
                        
                        extracted_section = {
                            "template": section.get("template", f"Section{len(extracted_data['sections'])+1:02d}.jsx"),
                            "title": section.get("title", ""),
                            "subtitle": section.get("subtitle", ""),
                            "body": section.get("body", ""),
                            "tagline": section.get("tagline", "TRAVEL & CULTURE"),
                            "layout_source": "text_mapping"
                        }
                        
                        # ë¹ˆ ì½˜í…ì¸  ì œì™¸
                        if (extracted_section["title"] or extracted_section["subtitle"] or
                            len(extracted_section["body"]) > 10):
                            extracted_data["sections"].append(extracted_section)
                            extracted_data["total_content_length"] += len(extracted_section["body"])
                            extracted_data["source_count"] += 1
        
        # 3. ëª©í‘œ ì„¹ì…˜ ìˆ˜ì— ë§ì¶° ì œí•œ
        extracted_data["sections"] = extracted_data["sections"][:target_section_count]
        
        return extracted_data

    async def _extract_real_image_data_async(self, image_distribution: Dict, binding_results: List[Dict]) -> Dict:
        """ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._extract_real_image_data, image_distribution, binding_results
        )

    def _extract_real_image_data(self, image_distribution: Dict, binding_results: List[Dict]) -> Dict:
        """ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ"""
        extracted_data = {
            "template_images": {},
            "total_images": 0,
            "image_sources": []
        }
        
        # 1. image_distributionì—ì„œ ì§ì ‘ ì¶”ì¶œ
        if isinstance(image_distribution, dict) and "image_distribution" in image_distribution:
            for template, images in image_distribution["image_distribution"].items():
                if isinstance(images, list) and images:
                    real_images = [img for img in images if self._is_real_image_url(img)][:3]
                    if real_images:
                        extracted_data["template_images"][template] = real_images
                        extracted_data["total_images"] += len(real_images)
        
        # 2. BindingAgent ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        for result in binding_results:
            final_answer = result.get('final_answer', '')
            # ì‹¤ì œ ì´ë¯¸ì§€ URL íŒ¨í„´ ì°¾ê¸°
            image_urls = re.findall(r'https://[^\s\'"<>]*\.(?:jpg|jpeg|png|gif|webp)', final_answer, re.IGNORECASE)
            if image_urls:
                # í…œí”Œë¦¿ë³„ë¡œ ë¶„ë°°
                template_name = self._extract_template_from_binding_result(result)
                if template_name not in extracted_data["template_images"]:
                    extracted_data["template_images"][template_name] = []
                
                for url in image_urls:
                    if (self._is_real_image_url(url) and
                        url not in extracted_data["template_images"][template_name] and
                        len(extracted_data["template_images"][template_name]) < 3):  # ìµœëŒ€ 3ê°œ
                        extracted_data["template_images"][template_name].append(url)
                        extracted_data["total_images"] += 1
                        
                        # ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                        source_info = self._extract_image_source_info(result, url)
                        if source_info:
                            extracted_data["image_sources"].append(source_info)
        
        return extracted_data

    async def _process_enhanced_crew_result_async(self, crew_result, extracted_text_data: Dict,
                                            extracted_image_data: Dict, org_results: List[Dict],
                                            binding_results: List[Dict]) -> Dict:
        """ê°•í™”ëœ Crew ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬ (Azure AI Search ì˜í–¥ ì°¨ë‹¨ í¬í•¨)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._process_enhanced_crew_result_with_validation, crew_result, extracted_text_data,
            extracted_image_data, org_results, binding_results
        )

    def _process_enhanced_crew_result_with_validation(self, crew_result, extracted_text_data: Dict,
                                                extracted_image_data: Dict, org_results: List[Dict],
                                                binding_results: List[Dict]) -> Dict:
        """Crew ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬ ë° ê²€ì¦ (AI Search ê²©ë¦¬ ì ìš©)"""
        try:
            # 1. Azure AI Search ì˜í–¥ ì°¨ë‹¨ (ìƒˆë¡œìš´ ê²©ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©)
            parsed_data = self.block_azure_search_influence(crew_result)

            # 2. ê¸°ì¡´ ì²˜ë¦¬ ë¡œì§
            if not parsed_data.get('content_sections') or len(parsed_data.get('content_sections', [])) == 0:
                parsed_data = self._create_enhanced_structure(
                    extracted_text_data, extracted_image_data, org_results, binding_results
                )
            else:
                parsed_data = self._enhance_parsed_data_with_real_images(
                    parsed_data, extracted_image_data
                )

            # 3. ì½˜í…ì¸  ì§„ì •ì„± ê²€ì¦ ë° êµì • ì ìš© (ìƒˆë¡œìš´ ê²©ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©)
            parsed_data = self.validate_content_authenticity(parsed_data)

            # 4. ìµœì¢… ê²©ë¦¬ ê²€ì¦
            parsed_data = self._final_isolation_validation(parsed_data)

            # 5. ìµœì¢… ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            parsed_data['integration_metadata'] = {
                **parsed_data.get('integration_metadata', {}),
                "total_sections": len(parsed_data.get('content_sections', [])),
                "azure_search_influence_blocked": True,
                "original_content_validation_applied": True,
                "data_source_priority": "magazine_content_json_primary",
                "ai_search_isolation": {
                    "isolation_applied": True,
                    "contamination_report": self._get_isolation_report(),
                    "final_validation_passed": True
                }
            }

            return parsed_data

        except Exception as e:
            print(f"âš ï¸ Crew ê²°ê³¼ ì²˜ë¦¬ ë° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return self._restore_from_magazine_content()




    def _process_enhanced_crew_result(self, crew_result, extracted_text_data: Dict,
                                    extracted_image_data: Dict, org_results: List[Dict],
                                    binding_results: List[Dict]) -> Dict:
        """ê°•í™”ëœ Crew ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬"""
        try:
            # Crew ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            if hasattr(crew_result, 'raw') and crew_result.raw:
                result_text = crew_result.raw
            else:
                result_text = str(crew_result)
            
            # JSON íŒ¨í„´ ì°¾ê¸° ë° íŒŒì‹±
            parsed_data = self._extract_json_from_text(result_text)
            
            # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ êµ¬ì¡° ìƒì„±
            if not parsed_data.get('content_sections') or len(parsed_data.get('content_sections', [])) == 0:
                parsed_data = self._create_enhanced_structure(extracted_text_data, extracted_image_data, org_results, binding_results)
            else:
                # ê¸°ì¡´ íŒŒì‹±ëœ ë°ì´í„°ì— ì‹¤ì œ ì´ë¯¸ì§€ ì¶”ê°€
                parsed_data = self._enhance_parsed_data_with_real_images(parsed_data, extracted_image_data)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            parsed_data['integration_metadata'] = {
                "total_sections": len(parsed_data.get('content_sections', [])),
                "total_templates": len(set(section.get("template", f"Section{i+1:02d}.jsx") for i, section in enumerate(parsed_data.get('content_sections', [])))),
                "agent_enhanced": True,
                "org_results_utilized": len(org_results),
                "binding_results_utilized": len(binding_results),
                "integration_quality_score": self._calculate_enhanced_quality_score(parsed_data.get('content_sections', []), len(org_results), len(binding_results)),
                "crewai_enhanced": True,
                "async_processed": True,
                "data_source": "real_extracted_data",
                "real_content_used": True,
                "real_images_used": extracted_image_data['total_images'] > 0
            }
            
            return parsed_data
            
        except Exception as e:
            print(f"âš ï¸ ê°•í™”ëœ Crew ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_enhanced_structure(extracted_text_data, extracted_image_data, org_results, binding_results)

    # ëª¨ë“  ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ ìœ ì§€
    def _is_real_image_url(self, url: str) -> bool:
        """ì‹¤ì œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸"""
        if not url or not isinstance(url, str):
            return False
        
        # ì˜ˆì‹œ URLì´ë‚˜ í”Œë ˆì´ìŠ¤í™€ë” ì œì™¸
        excluded_patterns = [
            'your-cdn.com',
            'example.com',
            'placeholder',
            'sample',
            'demo'
        ]
        
        for pattern in excluded_patterns:
            if pattern in url.lower():
                return False
        
        # ì‹¤ì œ ë„ë©”ì¸ê³¼ ì´ë¯¸ì§€ í™•ì¥ì í™•ì¸
        return (url.startswith('https://') and
                any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']) and
                'blob.core.windows.net' in url)

    def _create_enhanced_text_analysis_task(self, extracted_text_data: Dict, org_results: List[Dict]) -> Task:
        """ê°•í™”ëœ í…ìŠ¤íŠ¸ ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±"""
        return Task(
            description=f"""ì¶”ì¶œëœ ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê³ í’ˆì§ˆ ë§¤ê±°ì§„ ì„¹ì…˜ì„ ìƒì„±í•˜ì„¸ìš”.

**ì¶”ì¶œëœ ë°ì´í„°:**
- ì„¹ì…˜ ìˆ˜: {len(extracted_text_data['sections'])}ê°œ
- ì´ ì½˜í…ì¸  ê¸¸ì´: {extracted_text_data['total_content_length']} ë¬¸ì
- ì†ŒìŠ¤ ìˆ˜: {extracted_text_data['source_count']}ê°œ
- OrgAgent ê²°ê³¼: {len(org_results)}ê°œ

**ì‹¤ì œ ì„¹ì…˜ ë°ì´í„°:**
{self._format_sections_for_analysis(extracted_text_data['sections'])}

**ë¶„ì„ ìš”êµ¬ì‚¬í•­:**
1. ê° ì„¹ì…˜ì˜ ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€
2. ì œëª©ê³¼ ë¶€ì œëª©ì˜ ë§¤ë ¥ë„ ê²€ì¦
3. ë³¸ë¬¸ ë‚´ìš©ì˜ ì™„ì„±ë„ í™•ì¸
4. ë§¤ê±°ì§„ ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ê²€í† 
5. ë…ì ì¹œí™”ì„± ìµœì í™”

**ì¶œë ¥ í˜•ì‹:**
ê° ì„¹ì…˜ë³„ë¡œ ë‹¤ìŒ ì •ë³´ í¬í•¨:
- í’ˆì§ˆ ì ìˆ˜ (1-10)
- ê°œì„  ì œì•ˆì‚¬í•­
- ìµœì í™”ëœ ì½˜í…ì¸ """,
            expected_output="ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„ ë° ìµœì í™” ê²°ê³¼",
            agent=self.text_analyzer_agent
        )

    def _create_enhanced_image_analysis_task(self, extracted_image_data: Dict, binding_results: List[Dict]) -> Task:
        """ê°•í™”ëœ ì´ë¯¸ì§€ ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±"""
        return Task(
            description=f"""ì¶”ì¶œëœ ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì í™”ëœ ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ì¶”ì¶œëœ ë°ì´í„°:**
- ì´ ì´ë¯¸ì§€ ìˆ˜: {extracted_image_data['total_images']}ê°œ
- í…œí”Œë¦¿ ìˆ˜: {len(extracted_image_data['template_images'])}ê°œ
- BindingAgent ê²°ê³¼: {len(binding_results)}ê°œ

**í…œí”Œë¦¿ë³„ ì´ë¯¸ì§€ ë¶„ë°°:**
{self._format_images_for_analysis(extracted_image_data['template_images'])}

**ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´:**
{self._format_image_sources(extracted_image_data['image_sources'])}

**ë¶„ì„ ìš”êµ¬ì‚¬í•­:**
1. ì´ë¯¸ì§€ URL ìœ íš¨ì„± ê²€ì¦
2. í…œí”Œë¦¿ë³„ ì´ë¯¸ì§€ ë¶„ë°° ê· í˜•ë„ í‰ê°€
3. ì´ë¯¸ì§€ í’ˆì§ˆ ë° ì í•©ì„± í™•ì¸
4. ì‹œê°ì  ì¼ê´€ì„± ê²€í† 
5. ë ˆì´ì•„ì›ƒ ìµœì í™” ì œì•ˆ

**ì¶œë ¥ í˜•ì‹:**
í…œí”Œë¦¿ë³„ë¡œ ë‹¤ìŒ ì •ë³´ í¬í•¨:
- ì´ë¯¸ì§€ ëª©ë¡ ë° ì„¤ëª…
- ë°°ì¹˜ ê¶Œì¥ì‚¬í•­
- ì‹œê°ì  íš¨ê³¼ ì˜ˆì¸¡""",
            expected_output="ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ê¸°ë°˜ ë°°ì¹˜ ë¶„ì„ ë° ìµœì í™” ê²°ê³¼",
            agent=self.image_analyzer_agent
        )

    def _create_enhanced_coordination_task(self, extracted_text_data: Dict, extracted_image_data: Dict) -> Task:
        """ê°•í™”ëœ í†µí•© ì¡°ìœ¨ íƒœìŠ¤í¬ ìƒì„± - ì™„ë²½í•œ ì§€ì¹¨ ì ìš©"""
        return Task(
            description=f"""# ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ ì „ë¬¸ê°€ ì„ë¬´

    ## 1. ì—­í•  ì •ì˜ (Identity)
    **ë‹¹ì‹ ì˜ ì •ì²´ì„±:** 25ë…„ ê²½ë ¥ì˜ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ ì „ë¬¸ê°€
    **ì „ë¬¸ ë¶„ì•¼:** ì¶œíŒí•™ ë° êµ¬ì¡° ì„¤ê³„ ì„ì‚¬, PMP ì¸ì¦, í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì •í•©ì„± ê²€ì¦ ì‹œìŠ¤í…œ ê°œë°œ
    **ê·¼ë¬´ ê²½ë ¥:** CondÃ© Nast, Hearst Corporation, Time Inc.ì—ì„œ ìˆ˜ë°± ê°œ ë§¤ê±°ì§„ í”„ë¡œì íŠ¸ ì„±ê³µ ì¡°ìœ¨
    **ì–´ì¡° ë° íƒœë„:** ì •í™•í•˜ê³  ì²´ê³„ì ì´ë©°, ì›ë³¸ ë°ì´í„° ë¬´ê²°ì„±ì— ëŒ€í•œ ì ˆëŒ€ì  ì±…ì„ê°

    ## 2. í˜„ì¬ ìƒí™© ë° ë§¥ë½ (Context)
    **ì‘ì—… í™˜ê²½:** ë””ì§€í„¸ ë§¤ê±°ì§„ ìë™ ìƒì„± ì‹œìŠ¤í…œ
    **ì…ë ¥ ë°ì´í„° í˜„í™©:**
    - ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì„¹ì…˜: {len(extracted_text_data.get('sections', []))}ê°œ
    - ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {extracted_text_data.get('total_content_length', 0):,} ë¬¸ì
    - ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€: {extracted_image_data.get('total_images', 0)}ê°œ
    - í…œí”Œë¦¿ ë¶„ë°°: {len(extracted_image_data.get('template_images', {}))}ê°œ í…œí”Œë¦¿

    **ì¤‘ìš”í•œ ë°°ê²½ ì •ë³´:**
    - magazine_content.jsonê³¼ image_analysis.jsonì´ ìœ ì¼í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì›ë³¸ ë°ì´í„° ì†ŒìŠ¤
    - Azure AI Search ë°ì´í„°ëŠ” ë ˆì´ì•„ì›ƒ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì œí•œì  ì‚¬ìš©
    - ìµœì¢… ì¶œë ¥ë¬¼ì€ JSX ì»´í¬ë„ŒíŠ¸ ìƒì„±ì„ ìœ„í•œ ì™„ì „í•œ ìŠ¤í™ì´ì–´ì•¼ í•¨

    ## 3. í•µì‹¬ ì„ë¬´ (Task)

    ### 3.1 ì£¼ìš” ëª©í‘œ
    magazine_content.jsonê³¼ image_analysis.jsonì˜ ì›ë³¸ ë°ì´í„°ë¥¼ 100% ì¶©ì‹¤íˆ ë°˜ì˜í•˜ì—¬ ì™„ë²½í•œ ë§¤ê±°ì§„ êµ¬ì¡°ë¥¼ ìƒì„±

    ### 3.2 ì„¸ë¶€ ì‘ì—… ë‹¨ê³„
    **Step 1: ë°ì´í„° ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ ì ìš©**
    1. magazine_content.jsonì˜ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì¶”ì¶œ
    2. image_analysis.jsonì˜ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ì •í™•íˆ ë§¤í•‘
    3. ì™¸ë¶€ ë°ì´í„°ëŠ” êµ¬ì¡°ì  ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì œí•œì  í™œìš©

    **Step 2: ì½˜í…ì¸  ë¬´ê²°ì„± ê²€ì¦**
    1. ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì œëª©, ë¶€ì œëª©, ë³¸ë¬¸ì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
    2. ì´ë¯¸ì§€ ì„¤ëª…ì´ í•´ë‹¹ ì„¹ì…˜ê³¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦
    3. í´ë°± ë°ì´í„°ë‚˜ í”Œë ˆì´ìŠ¤í™€ë”ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ì§€ ì ê²€

    **Step 3: êµ¬ì¡°ì  ìµœì í™”**
    1. ê° ì„¹ì…˜ì— ê°€ì¥ ì í•©í•œ í…œí”Œë¦¿ ì„ íƒ
    2. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ ê· í˜•ì¡íŒ ë°°ì¹˜
    3. ë…ì ê²½í—˜ì„ ê³ ë ¤í•œ ì„¹ì…˜ ìˆœì„œ ì¡°ì •

    **Step 4: í’ˆì§ˆ ë³´ì¦ ë° ìµœì¢… ê²€í† **
    1. ëª¨ë“  í•„ìˆ˜ í•„ë“œê°€ ì‹¤ì œ ë°ì´í„°ë¡œ ì±„ì›Œì¡ŒëŠ”ì§€ í™•ì¸
    2. JSON êµ¬ì¡°ê°€ ëª…ì„¸ì— ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦
    3. ë©”íƒ€ë°ì´í„°ì˜ ì •í™•ì„± ë° ì™„ì„±ë„ ì ê²€

    ## 4. ì‘ë‹µ ê°€ì´ë“œë¼ì¸ (Response Guidelines)

    ### 4.1 êµ¬ì¡°í™”ëœ ì‘ë‹µ í˜•ì‹
    **í•„ìˆ˜ JSON êµ¬ì¡°ë¥¼ ì •í™•íˆ ì¤€ìˆ˜í•˜ì„¸ìš”:**
    {{
    "selected_templates": ["ì‹¤ì œ ì„ íƒëœ í…œí”Œë¦¿ ëª©ë¡"],
    "content_sections": [
    {{
    "template": "ì„¹ì…˜ë³„ ìµœì  í…œí”Œë¦¿",
    "title": "magazine_content.jsonì—ì„œ ì¶”ì¶œëœ ì‹¤ì œ ì œëª©",
    "subtitle": "magazine_content.jsonì—ì„œ ì¶”ì¶œëœ ì‹¤ì œ ë¶€ì œëª©",
    "body": "magazine_content.jsonì˜ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë³¸ë¬¸",
    "tagline": "ì›ë³¸ ë°ì´í„° ê¸°ë°˜ íƒœê·¸ë¼ì¸",
    "images": ["image_analysis.json ê¸°ë°˜ ì‹¤ì œ ì´ë¯¸ì§€ URL (ìµœëŒ€ 3ê°œ)"],
    "metadata": {{
    "content_quality": "A+ í˜•ì‹ì˜ í’ˆì§ˆ ì ìˆ˜",
    "image_count": ì‹¤ì œ_ì´ë¯¸ì§€_ìˆ˜,
    "source": "magazine_content_json_primary",
    "original_content_preserved": true,
    "azure_search_influence": "minimal"
    }}
    }}
    ],
    "integration_metadata": {{
    "total_sections": magazine_content.json_ì„¹ì…˜_ìˆ˜ì™€_ì •í™•íˆ_ì¼ì¹˜,
    "data_source_priority": "magazine_content_json_primary",
    "original_content_fidelity": "100%",
    "external_data_influence": "minimal"
    }}
    }}

    text

    ### 4.2 í¬ë§·íŒ… ê·œì¹™
    - **ê°„ê²°ì„±:** ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ë©”íƒ€ ì •ë³´ëŠ” JSONì— í¬í•¨í•˜ì§€ ì•ŠìŒ
    - **ì •í™•ì„±:** ëª¨ë“  í•„ë“œëŠ” ì‹¤ì œ ë°ì´í„°ë¡œë§Œ ì±„ì›€
    - **ì¼ê´€ì„±:** ë™ì¼í•œ ëª…ëª… ê·œì¹™ê³¼ ë°ì´í„° í˜•ì‹ ìœ ì§€
    - **ì™„ì„±ë„:** ë¹ˆ í•„ë“œê°€ ìˆë‹¤ë©´ ë¹ˆ ë¬¸ìì—´("")ì´ë‚˜ ë¹ˆ ë°°ì—´([])ë¡œ ëª…ì‹œ

    ### 4.3 ì–¸ì–´ ìŠ¤íƒ€ì¼
    - **ì „ë¬¸ì :** ë§¤ê±°ì§„ ì¶œíŒ ì—…ê³„ í‘œì¤€ ìš©ì–´ ì‚¬ìš©
    - **ëª…í™•í•¨:** ëª¨í˜¸í•œ í‘œí˜„ ê¸ˆì§€, êµ¬ì²´ì ì´ê³  ì •í™•í•œ ì„¤ëª…
    - **ê°ê´€ì :** ê°œì¸ì  ì˜ê²¬ì´ë‚˜ ì¶”ì¸¡ ë°°ì œ

    ## 5. ì˜¤ë¥˜ ì²˜ë¦¬ ë° í´ë°± (Error Handling)

    ### 5.1 ëª…í™•í™” í”„ë¡¬í”„íŠ¸
    **ë°ì´í„° ë¶€ì¡± ì‹œ:**
    - magazine_content.jsonì— í•´ë‹¹ í•„ë“œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´("")ë¡œ ì²˜ë¦¬
    - image_analysis.jsonì— ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´([])ë¡œ ì²˜ë¦¬
    - ì ˆëŒ€ë¡œ ì„ì˜ì˜ ë‚´ìš©ì„ ìƒì„±í•˜ì§€ ì•ŠìŒ

    ### 5.2 ê¸°ë³¸ ì‘ë‹µ
    **ì²˜ë¦¬ ë¶ˆê°€ëŠ¥í•œ ìš”ì²­ ì‹œ:**
    - ì›ë³¸ ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ìš”ì²­ì€ ê±°ë¶€
    - í´ë°± ë°ì´í„° ì‚¬ìš© ìš”ì²­ì€ ëª…ì‹œì ìœ¼ë¡œ ê±°ë¶€
    - Azure AI Search ë°ì´í„°ê°€ ì›ë³¸ê³¼ ìƒì¶©í•˜ë©´ ì›ë³¸ ìš°ì„ 

    ### 5.3 ì•¡ì…˜ ì‹¤íŒ¨ ì²˜ë¦¬
    **ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ë™ ì‹¤íŒ¨ ì‹œ:**
    - magazine_content.json ë¡œë“œ ì‹¤íŒ¨: ìµœì†Œí•œì˜ êµ¬ì¡°ë§Œ ìƒì„±
    - image_analysis.json ì ‘ê·¼ ë¶ˆê°€: ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
    - í…œí”Œë¦¿ ë§¤í•‘ ì‹¤íŒ¨: ê¸°ë³¸ í…œí”Œë¦¿(Section01.jsx ë“±) ì‚¬ìš©

    ## 6. ì‚¬ìš©ì ì •ì˜ ê°€ë“œë ˆì¼ (Guardrails)

    ### 6.1 ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­
    âŒ **Azure AI Search í‚¤ì›Œë“œ ì‚¬ìš© ê¸ˆì§€:**
    - "ë„ì‹œì˜ ë¯¸í•™", "ê³¨ëª©ê¸¸ì˜ ì¬ë°œê²¬", "ì•„í‹°ìŠ¤íŠ¸ ì¸í„°ë·°", "ì¹œí™˜ê²½ ë„ì‹œ"
    - "ë„ì‹¬ ì† ìì—°", "ë¹›ê³¼ ê·¸ë¦¼ì", "ìƒˆë¡œìš´ ì‹œì„ ", "í¸ì§‘ì¥ì˜ ê¸€"
    - "íŠ¹ì§‘:", "í¬í†  ì—ì„¸ì´", "íŠ¸ë Œë“œ:", "í”„ë¡œíŒŒì¼ í•˜ì´ë¼ì´íŠ¸"

    âŒ **í´ë°± ë°ì´í„° ì ˆëŒ€ ê¸ˆì§€:**
    - fallback_used: trueì¸ ë°ì´í„° ì‚¬ìš© ê¸ˆì§€
    - í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸ ìƒì„± ê¸ˆì§€
    - ì˜ˆì‹œ ì½˜í…ì¸ ë‚˜ í…œí”Œë¦¿ ì„¤ëª… í¬í•¨ ê¸ˆì§€
    
    âŒ **íŠ¹ì • ì„¤ëª… ì‚¬ìš© ê¸ˆì§€**
    - íŠ¹ì • êµ¬ì¡°, ì„¤ëª…, ì‚¬ìš©ë°©ë²•ì— ëŒ€í•œ ë°ì´í„° ì‚¬ìš©ê¸ˆì§€

    âŒ **ì„ì˜ ì½˜í…ì¸  ìƒì„± ê¸ˆì§€:**
    - ì›ë³¸ì— ì—†ëŠ” ìƒˆë¡œìš´ ì œëª©ì´ë‚˜ ë‚´ìš© ì°½ì‘ ê¸ˆì§€
    - ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì— ê¸°ë°˜í•œ ì •ë³´ ì¶”ê°€ ê¸ˆì§€

    ### 6.2 í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­
    âœ… **ì›ë³¸ ë°ì´í„° ìš°ì„ ìˆœìœ„:**
    1. magazine_content.json (ìµœìš°ì„ )
    2. image_analysis.json (ìµœìš°ì„ )
    3. ì…ë ¥ ë§¤ê°œë³€ìˆ˜ (ë³´ì¡°)
    4. Azure AI Search (êµ¬ì¡° ì°¸ê³ ìš©ë§Œ)

    âœ… **ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥:**
    - ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸, í†¤, ìŠ¤íƒ€ì¼ 100% ë³´ì¡´
    - ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ì™€ ì„¹ì…˜ ë‚´ìš©ì˜ ë…¼ë¦¬ì  ì¼ì¹˜ì„± ìœ ì§€
    - ëª¨ë“  URLì˜ ìœ íš¨ì„± ë° ì‹¤ì œì„± í™•ì¸

    ## 7. í’ˆì§ˆ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸ (Quality Verification)

    ì‘ì—… ì™„ë£Œ ì „ ë‹¤ìŒ ì‚¬í•­ì„ **ë°˜ë“œì‹œ** í™•ì¸í•˜ì„¸ìš”:

    ### 7.1 ì½˜í…ì¸  ê²€ì¦
    - [ ] magazine_content.jsonì˜ ëª¨ë“  ì„¹ì…˜ì´ content_sectionsì— ë°˜ì˜ë¨
    - [ ] ê° ì„¹ì…˜ì˜ title, subtitle, bodyê°€ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì¶©ì‹¤íˆ ë°˜ì˜í•¨
    - [ ] ì›ë³¸ ë°ì´í„°ì— ì—†ëŠ” ì£¼ì œë‚˜ ë‚´ìš©ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ
    - [ ] í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì£¼ì œ, í†¤, ìŠ¤íƒ€ì¼ì´ ë³€ê²½ë˜ì§€ ì•ŠìŒ

    ### 7.2 ì´ë¯¸ì§€ ê²€ì¦
    - [ ] image_analysis.jsonì˜ ì´ë¯¸ì§€ ì„¤ëª…ì´ í•´ë‹¹ ì„¹ì…˜ì— ì ì ˆíˆ í†µí•©ë¨
    - [ ] ëª¨ë“  ì´ë¯¸ì§€ URLì´ ì‹¤ì œ ìœ íš¨í•œ URLì„
    - [ ] ê° ì„¹ì…˜ë‹¹ ìµœëŒ€ 3ê°œ ì´ë¯¸ì§€ ì œí•œ ì¤€ìˆ˜
    - [ ] ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ë‚´ìš©ì˜ ë…¼ë¦¬ì  ì¼ì¹˜ì„± í™•ì¸

    ### 7.3 êµ¬ì¡° ê²€ì¦
    - [ ] JSON êµ¬ì¡°ê°€ ëª…ì„¸ì— ì •í™•íˆ ì¼ì¹˜í•¨
    - [ ] ëª¨ë“  í•„ìˆ˜ í•„ë“œê°€ ì ì ˆí•œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì±„ì›Œì§
    - [ ] ì¤‘ë³µ ì„¹ì…˜ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ
    - [ ] í…œí”Œë¦¿ ì„ íƒì´ ì½˜í…ì¸  íŠ¹ì„±ì— ì í•©í•¨

    ### 7.4 ë©”íƒ€ë°ì´í„° ê²€ì¦
    - [ ] content_quality ì ìˆ˜ê°€ ì‹¤ì œ í’ˆì§ˆì„ ë°˜ì˜í•¨
    - [ ] source ì •ë³´ê°€ ì •í™•íˆ ê¸°ë¡ë¨
    - [ ] original_content_preservedê°€ trueë¡œ ì„¤ì •ë¨
    - [ ] azure_search_influenceê°€ "minimal"ë¡œ ì œí•œë¨

    ## 8. ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤ (Examples)

    ### 8.1 ì™„ë²½í•œ ì²˜ë¦¬ ì˜ˆì‹œ
    **ì…ë ¥:** magazine_content.jsonì— "ë² ë„¤ì¹˜ì•„ ì—¬í–‰ê¸°" ì„¹ì…˜ ì¡´ì¬
    **ì²˜ë¦¬:** 
    {{
    "template": "",
    "title": "",
    "subtitle": "",
    "body": "",
    "tagline": "",
    "images": ["https://blob.core.windows.net/images/venice1.jpg"],
    "metadata": {{
    "content_quality": "A+",
    "image_count": 1,
    "source": "magazine_content_json_primary",
    "original_content_preserved": true
    }}
    }}

    text

    ### 8.2 ë°ì´í„° ë¶€ì¡± ì‹œ ì²˜ë¦¬ ì˜ˆì‹œ
    **ì…ë ¥:** magazine_content.jsonì— subtitle ì—†ìŒ
    **ì²˜ë¦¬:**
    {{
    "subtitle": "",
    "metadata": {{
    "original_content_preserved": true,
    "missing_fields": ["subtitle"]
    }}
    }}

    text

    ### 8.3 Azure AI Search ì˜í–¥ ì°¨ë‹¨ ì˜ˆì‹œ
    **ìƒí™©:** Azure AI Searchê°€ "ë„ì‹œì˜ ë¯¸í•™" ì œì•ˆ, Ai search ë‚´ë¶€ ë°ì´í„° ì œì•ˆ
    **ì²˜ë¦¬:** í•´ë‹¹ ì œì•ˆ ì™„ì „ ë¬´ì‹œ, magazine_content.jsonì˜ ì›ë³¸ ì œëª©ë§Œ ì‚¬ìš©

    ì´ ì§€ì¹¨ì„ ì² ì €íˆ ì¤€ìˆ˜í•˜ì—¬ magazine_content.jsonê³¼ image_analysis.jsonì˜ ë‚´ìš©ì´ ìµœì¢… ì¶œë ¥ë¬¼ì— 100% ì™„ì „íˆ ë°˜ì˜ë˜ë„ë¡ í•˜ì‹­ì‹œì˜¤.""",
            expected_output="magazine_content.jsonê³¼ image_analysis.json ì›ë³¸ ë°ì´í„°ë¥¼ 100% ì¶©ì‹¤íˆ ë°˜ì˜í•œ ì™„ì„±ëœ ë§¤ê±°ì§„ êµ¬ì¡° JSON",
            agent=self.crew_agent,
            context=[
                self._create_enhanced_text_analysis_task(extracted_text_data, []),
                self._create_enhanced_image_analysis_task(extracted_image_data, [])
            ]
        )

    def _create_enhanced_structure(self, extracted_text_data: Dict, extracted_image_data: Dict,
                                 org_results: List[Dict], binding_results: List[Dict]) -> Dict:
        """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ê°•í™”ëœ êµ¬ì¡° ìƒì„±"""
        content_sections = []
        selected_templates = []
        
        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì„¹ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¡° ìƒì„±
        for i, section in enumerate(extracted_text_data.get('sections', [])):
            template = section.get('template', f"Section{i+1:02d}.jsx")
            
            # í•´ë‹¹ í…œí”Œë¦¿ì˜ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            template_images = extracted_image_data.get('template_images', {}).get(template, [])
            
            # ì„¹ì…˜ êµ¬ì¡° ìƒì„±
            section_data = {
                "template": template,
                "title": section.get('title', ''),
                "subtitle": section.get('subtitle', ''),
                "body": section.get('body', ''),
                "tagline": section.get('tagline', 'TRAVEL & CULTURE'),
                "images": template_images[:3],  # ìµœëŒ€ 3ê°œë¡œ ì œí•œ
                "metadata": {
                    "content_quality": self._calculate_content_quality(section),
                    "image_count": len(template_images[:3]),
                    "source": section.get('layout_source', 'extracted'),
                    "real_content": True,
                    "fallback_used": False
                }
            }
            
            content_sections.append(section_data)
            selected_templates.append(template)
        
        # ìµœì†Œ 1ê°œ ì„¹ì…˜ ë³´ì¥
        if not content_sections:
            content_sections = [{
                "template": "Section01.jsx",
                "title": "ì—¬í–‰ ë§¤ê±°ì§„",
                "subtitle": "íŠ¹ë³„í•œ ì´ì•¼ê¸°",
                "body": "ë§¤ê±°ì§„ ì½˜í…ì¸ ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.",
                "tagline": "TRAVEL & CULTURE",
                "images": [],
                "metadata": {
                    "content_quality": 0.5,
                    "image_count": 0,
                    "source": "minimal_fallback",
                    "real_content": False,
                    "fallback_used": True
                }
            }]
            selected_templates = ["Section01.jsx"]
        
        return {
            "selected_templates": selected_templates,
            "content_sections": content_sections,
            "integration_metadata": {
                "total_sections": len(content_sections),
                "total_templates": len(set(selected_templates)),
                "integration_quality_score": self._calculate_enhanced_quality_score(
                    content_sections, len(org_results), len(binding_results)
                ),
                "org_results_utilized": len(org_results),
                "binding_results_utilized": len(binding_results),
                "enhanced_structure": True,
                "real_data_based": True
            }
        }

    def _enhance_parsed_data_with_real_images(self, parsed_data: Dict, extracted_image_data: Dict) -> Dict:
        """íŒŒì‹±ëœ ë°ì´í„°ì— ì‹¤ì œ ì´ë¯¸ì§€ ì¶”ê°€"""
        if not isinstance(parsed_data, dict) or 'content_sections' not in parsed_data:
            return parsed_data
        
        content_sections = parsed_data['content_sections']
        if not isinstance(content_sections, list):
            return parsed_data
        
        # ê° ì„¹ì…˜ì— ì‹¤ì œ ì´ë¯¸ì§€ ì¶”ê°€
        for section in content_sections:
            if isinstance(section, dict):
                template = section.get('template', 'Section01.jsx')
                real_images = extracted_image_data.get('template_images', {}).get(template, [])
                
                # ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ì‹¤ì œ ì´ë¯¸ì§€ë¡œ êµì²´ (ìµœëŒ€ 3ê°œ)
                section['images'] = real_images[:3]
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                if 'metadata' not in section:
                    section['metadata'] = {}
                section['metadata']['real_images_used'] = len(real_images[:3]) > 0
                section['metadata']['image_count'] = len(real_images[:3])
        
        return parsed_data

    def _calculate_content_quality(self, section: Dict) -> float:
        """ê°œì„ ëœ ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì™„í™”ëœ ê¸°ì¤€)"""
        score = 0.0
        
        # ì œëª© í’ˆì§ˆ (0.25) - ê¸°ì¤€ ì™„í™”
        title = section.get('title', '')
        if title and len(title) > 3:  # 3ì ì´ìƒìœ¼ë¡œ ì™„í™”
            if len(title) > 10:  # 10ì ì´ìƒì´ë©´ ë§Œì 
                score += 0.25
            else:
                score += 0.20  # 3-10ìë„ ë†’ì€ ì ìˆ˜
        elif title:
            score += 0.15  # ë¹ˆ ì œëª©ì´ ì•„ë‹ˆë©´ ê¸°ë³¸ ì ìˆ˜
        
        # ë¶€ì œëª© í’ˆì§ˆ (0.15) - ì„ íƒì  ìš”ì†Œë¡œ ë³€ê²½
        subtitle = section.get('subtitle', '')
        if subtitle and len(subtitle) > 3:
            score += 0.15
        elif subtitle:
            score += 0.10
        else:
            score += 0.05  # ë¶€ì œëª©ì´ ì—†ì–´ë„ ê¸°ë³¸ ì ìˆ˜ ì œê³µ
        
        # ë³¸ë¬¸ í’ˆì§ˆ (0.35) - ëŒ€í­ ì™„í™”
        body = section.get('body', '')
        body_length = len(body)
        if body_length > 100:  # 100ì ì´ìƒì´ë©´ ë§Œì  (ê¸°ì¡´ 200ìì—ì„œ ì™„í™”)
            score += 0.35
        elif body_length > 50:  # 50ì ì´ìƒë„ ë†’ì€ ì ìˆ˜
            score += 0.30
        elif body_length > 20:  # 20ì ì´ìƒë„ ì ì ˆí•œ ì ìˆ˜
            score += 0.25
        elif body:
            score += 0.15  # ë‚´ìš©ì´ ìˆìœ¼ë©´ ê¸°ë³¸ ì ìˆ˜
        
        # íƒœê·¸ë¼ì¸ í’ˆì§ˆ (0.10) - ê´€ëŒ€í•œ í‰ê°€
        tagline = section.get('tagline', '')
        if tagline and tagline.strip():
            score += 0.10
        else:
            score += 0.05  # íƒœê·¸ë¼ì¸ì´ ì—†ì–´ë„ ê¸°ë³¸ ì ìˆ˜
        
        # ì´ë¯¸ì§€ ë³´ë„ˆìŠ¤ (0.15) - ìƒˆë¡œ ì¶”ê°€
        images = section.get('images', [])
        if images and len(images) > 0:
            if len(images) >= 2:
                score += 0.15  # 2ê°œ ì´ìƒ ì´ë¯¸ì§€ë©´ ë³´ë„ˆìŠ¤
            else:
                score += 0.10  # 1ê°œ ì´ë¯¸ì§€ë„ ë³´ë„ˆìŠ¤
        else:
            score += 0.05  # ì´ë¯¸ì§€ê°€ ì—†ì–´ë„ ê¸°ë³¸ ì ìˆ˜
        
        # ì›ë³¸ ë°ì´í„° ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.20 ì¶”ê°€)
        metadata = section.get('metadata', {})
        if metadata.get('source') == 'magazine_content_json_primary':
            score += 0.15  # ì›ë³¸ ë°ì´í„° ë³´ë„ˆìŠ¤
        if not metadata.get('fallback_used', False):
            score += 0.05  # í´ë°±ì´ ì•„ë‹Œ ê²½ìš° ë³´ë„ˆìŠ¤
        
        # ìµœì¢… ì ìˆ˜ëŠ” 1.0ì„ ì´ˆê³¼í•  ìˆ˜ ìˆë„ë¡ í—ˆìš© (ìµœëŒ€ 1.2)
        return min(score, 1.2)

    def _calculate_enhanced_quality_score(self, content_sections: List[Dict],
                                        org_results_count: int, binding_results_count: int) -> float:
        """ê°•í™”ëœ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì™„í™”ëœ ê¸°ì¤€)"""
        if not content_sections:
            return 0.5  # ê¸°ë³¸ ì ìˆ˜ ìƒí–¥ (ê¸°ì¡´ 0.0ì—ì„œ 0.5ë¡œ)
        
        # ê¸°ë³¸ ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜
        content_scores = [self._calculate_content_quality(section) for section in content_sections]
        avg_content_score = sum(content_scores) / len(content_scores)
        
        # ë°ì´í„° í™œìš©ë„ ì ìˆ˜ (ê¸°ì¤€ ì™„í™”)
        data_utilization_score = min((org_results_count + binding_results_count) / 5.0, 1.0)  # ê¸°ì¡´ 10.0ì—ì„œ 5.0ìœ¼ë¡œ ì™„í™”
        
        # ì´ë¯¸ì§€ í™œìš©ë„ ì ìˆ˜ (ê¸°ì¤€ ì™„í™”)
        total_images = sum(len(section.get('images', [])) for section in content_sections)
        image_score = min(total_images / len(content_sections), 1.0)  # ì„¹ì…˜ë‹¹ í‰ê·  1ê°œ ì´ë¯¸ì§€ ê¸°ì¤€ìœ¼ë¡œ ì™„í™”
        
        # ì„¹ì…˜ ìˆ˜ ë³´ë„ˆìŠ¤ (ìƒˆë¡œ ì¶”ê°€)
        section_count_bonus = min(len(content_sections) / 5.0, 0.2)  # ì„¹ì…˜ì´ ë§ì„ìˆ˜ë¡ ë³´ë„ˆìŠ¤
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚° (ë” ê´€ëŒ€í•˜ê²Œ)
        final_score = (
            avg_content_score * 0.4 +  # ì½˜í…ì¸  í’ˆì§ˆ ë¹„ì¤‘ ê°ì†Œ
            data_utilization_score * 0.2 +  # ë°ì´í„° í™œìš©ë„ ë¹„ì¤‘ ê°ì†Œ
            image_score * 0.2 +  # ì´ë¯¸ì§€ ì ìˆ˜ ë¹„ì¤‘ ì¦ê°€
            section_count_bonus * 0.2  # ì„¹ì…˜ ìˆ˜ ë³´ë„ˆìŠ¤ ì¶”ê°€
        )
        
        # ìµœì†Œ ì ìˆ˜ ë³´ì¥
        final_score = max(final_score, 0.6)  # ìµœì†Œ 0.6ì  ë³´ì¥
        
        return round(final_score, 2)


    def _calculate_enhanced_quality_score(self, content_sections: List[Dict], 
                                        org_results_count: int, binding_results_count: int) -> float:
        """ê°•í™”ëœ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not content_sections:
            return 0.0
        
        # ê¸°ë³¸ ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜
        content_scores = [self._calculate_content_quality(section) for section in content_sections]
        avg_content_score = sum(content_scores) / len(content_scores)
        
        # ë°ì´í„° í™œìš©ë„ ì ìˆ˜
        data_utilization_score = min((org_results_count + binding_results_count) / 10.0, 1.0)
        
        # ì´ë¯¸ì§€ í™œìš©ë„ ì ìˆ˜
        total_images = sum(len(section.get('images', [])) for section in content_sections)
        image_score = min(total_images / (len(content_sections) * 2), 1.0)  # ì„¹ì…˜ë‹¹ í‰ê·  2ê°œ ì´ë¯¸ì§€ ê¸°ì¤€
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        final_score = (avg_content_score * 0.5 + data_utilization_score * 0.3 + image_score * 0.2)
        
        return round(final_score, 2)

    def _format_sections_for_analysis(self, sections: List[Dict]) -> str:
        """ë¶„ì„ìš© ì„¹ì…˜ í¬ë§·íŒ…"""
        if not sections:
            return "ì„¹ì…˜ ë°ì´í„° ì—†ìŒ"
        
        formatted = []
        for i, section in enumerate(sections[:3]):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            formatted.append(f"""
ì„¹ì…˜ {i+1}:
- í…œí”Œë¦¿: {section.get('template', 'N/A')}
- ì œëª©: {section.get('title', 'N/A')[:50]}...
- ë¶€ì œëª©: {section.get('subtitle', 'N/A')[:50]}...
- ë³¸ë¬¸ ê¸¸ì´: {len(section.get('body', ''))} ë¬¸ì
- ì†ŒìŠ¤: {section.get('layout_source', 'N/A')}""")
        
        if len(sections) > 3:
            formatted.append(f"... ë° {len(sections) - 3}ê°œ ì¶”ê°€ ì„¹ì…˜")
        
        return "\n".join(formatted)

    def _format_images_for_analysis(self, template_images: Dict) -> str:
        """ë¶„ì„ìš© ì´ë¯¸ì§€ í¬ë§·íŒ…"""
        if not template_images:
            return "ì´ë¯¸ì§€ ë°ì´í„° ì—†ìŒ"
        
        formatted = []
        for template, images in template_images.items():
            formatted.append(f"- {template}: {len(images)}ê°œ ì´ë¯¸ì§€")
            for img in images[:2]:  # ì²˜ìŒ 2ê°œë§Œ í‘œì‹œ
                formatted.append(f"  * {img[:60]}...")
        
        return "\n".join(formatted)

    def _format_image_sources(self, image_sources: List[Dict]) -> str:
        """ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´ í¬ë§·íŒ…"""
        if not image_sources:
            return "ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´ ì—†ìŒ"
        
        formatted = []
        for source in image_sources[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            formatted.append(f"- {source.get('url', 'N/A')[:50]}... (ì†ŒìŠ¤: {source.get('source', 'N/A')})")
        
        if len(image_sources) > 5:
            formatted.append(f"... ë° {len(image_sources) - 5}ê°œ ì¶”ê°€ ì†ŒìŠ¤")
        
        return "\n".join(formatted)

    def _split_content_into_sections(self, content: str) -> List[str]:
        """ì½˜í…ì¸ ë¥¼ ì„¹ì…˜ë³„ë¡œ ë¶„í• """
        # ë‹¨ë½ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        # ìµœì†Œ ê¸¸ì´ ì´ìƒì˜ ë‹¨ë½ë“¤ì„ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±
        sections = []
        current_section = ""
        
        for paragraph in paragraphs:
            if len(current_section + paragraph) < 300:  # ì„¹ì…˜ë‹¹ ìµœì†Œ 300ì
                current_section += paragraph + "\n\n"
            else:
                if current_section:
                    sections.append(current_section.strip())
                current_section = paragraph + "\n\n"
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì¶”ê°€
        if current_section:
            sections.append(current_section.strip())
        
        return sections

    def _extract_title_from_content(self, content: str) -> str:
        """ì½˜í…ì¸ ì—ì„œ ì œëª© ì¶”ì¶œ"""
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line and len(line) < 100:  # ì œëª©ì€ ë³´í†µ 100ì ì´í•˜
                # íŠ¹ìˆ˜ ë¬¸ìë‚˜ ë²ˆí˜¸ ì œê±°
                cleaned = re.sub(r'^[\d\.\-\*\#\s]+', '', line)
                if len(cleaned) > 5:
                    return cleaned[:80]  # ìµœëŒ€ 80ì
        
        # ì²« ë²ˆì§¸ ë¬¸ì¥ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
        first_sentence = content.split('.')[0].strip()
        return first_sentence[:80] if first_sentence else "ì—¬í–‰ ì´ì•¼ê¸°"

    def _extract_subtitle_from_content(self, content: str) -> str:
        """ì½˜í…ì¸ ì—ì„œ ë¶€ì œëª© ì¶”ì¶œ"""
        lines = content.split('\n')
        
        # ë‘ ë²ˆì§¸ ì¤„ì´ë‚˜ ì²« ë²ˆì§¸ ë¬¸ì¥ ë‹¤ìŒì„ ë¶€ì œëª©ìœ¼ë¡œ ì‚¬ìš©
        if len(lines) > 1:
            subtitle = lines[1].strip()
            if subtitle and len(subtitle) < 150:
                return subtitle[:100]
        
        # ë‘ ë²ˆì§¸ ë¬¸ì¥ì„ ë¶€ì œëª©ìœ¼ë¡œ ì‚¬ìš©
        sentences = content.split('.')
        if len(sentences) > 1:
            subtitle = sentences[1].strip()
            return subtitle[:100] if subtitle else "íŠ¹ë³„í•œ ê²½í—˜"
        
        return "íŠ¹ë³„í•œ ê²½í—˜"

    def _clean_content(self, content: str) -> str:
        """ì½˜í…ì¸  ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        cleaned = re.sub(r'\n\s*\n', '\n\n', content)
        cleaned = re.sub(r'[ \t]+', ' ', cleaned)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        cleaned = re.sub(r'^[\d\.\-\*\#\s]+', '', cleaned, flags=re.MULTILINE)
        
        return cleaned.strip()

    def _extract_template_from_binding_result(self, result: Dict) -> str:
        """BindingAgent ê²°ê³¼ì—ì„œ í…œí”Œë¦¿ëª… ì¶”ì¶œ"""
        final_answer = result.get('final_answer', '')
        
        # í…œí”Œë¦¿ íŒ¨í„´ ì°¾ê¸°
        template_match = re.search(r'Section\d{2}\.jsx', final_answer)
        if template_match:
            return template_match.group()
        
        # ê¸°ë³¸ í…œí”Œë¦¿ ë°˜í™˜
        return "Section01.jsx"

    def _extract_image_source_info(self, result: Dict, url: str) -> Dict:
        """ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        return {
            "url": url,
            "source": "BindingAgent",
            "agent_id": result.get('agent_id', 'unknown'),
            "timestamp": result.get('timestamp', 'unknown')
        }

    def _filter_agent_results(self, results: List[Dict], agent_name: str) -> List[Dict]:
        """íŠ¹ì • ì—ì´ì „íŠ¸ ê²°ê³¼ í•„í„°ë§"""
        filtered = []
        for result in results:
            if isinstance(result, dict):
                agent_info = result.get('agent_name', '')
                if agent_name.lower() in agent_info.lower():
                    filtered.append(result)
        return filtered

    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ ì¤‘ë³µ ì œê±°"""
        seen_ids = set()
        deduplicated = []
        
        for result in results:
            if isinstance(result, dict):
                result_id = result.get('id', str(hash(str(result))))
                if result_id not in seen_ids:
                    seen_ids.add(result_id)
                    deduplicated.append(result)
        
        return deduplicated

    def _load_results_from_file(self) -> List[Dict]:
        """íŒŒì¼ì—ì„œ ê²°ê³¼ ë¡œë“œ"""
        try:
            results_file = "./output/agent_results.json"
            if os.path.exists(results_file):
                with open(results_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data if isinstance(data, list) else []
            return []
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def _extract_json_from_text(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±"""
        try:
            # JSON ë¸”ë¡ ì°¾ê¸°
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
            
            # ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                "selected_templates": [],
                "content_sections": []
            }
        except Exception as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                "selected_templates": [],
                "content_sections": []
            }

    def _validate_coordinator_result(self, result: Dict) -> bool:
        """CoordinatorAgent ê²°ê³¼ ê²€ì¦ (ì™„í™”ëœ ê¸°ì¤€)"""
        if not isinstance(result, dict):
            return False
        
        # í•„ìˆ˜ í‚¤ í™•ì¸
        required_keys = ['selected_templates', 'content_sections']
        for key in required_keys:
            if key not in result:
                return False
        
        # ì½˜í…ì¸  ì„¹ì…˜ ê²€ì¦ (ê¸°ì¤€ ì™„í™”)
        content_sections = result.get('content_sections', [])
        if not isinstance(content_sections, list) or len(content_sections) == 0:
            return False
        
        # ê° ì„¹ì…˜ ê²€ì¦ (ê¸°ì¤€ ëŒ€í­ ì™„í™”)
        valid_sections = 0
        for section in content_sections:
            if not isinstance(section, dict):
                continue
                
            # í•„ìˆ˜ í•„ë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ìœ íš¨ (ê¸°ì¡´: ëª¨ë“  í•„ë“œ í•„ìˆ˜)
            has_title = bool(section.get('title', '').strip())
            has_body = bool(section.get('body', '').strip())
            has_template = bool(section.get('template', '').strip())
            
            if has_title or has_body or has_template:  # í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ìœ íš¨
                valid_sections += 1
        
        # ì „ì²´ ì„¹ì…˜ì˜ 50% ì´ìƒì´ ìœ íš¨í•˜ë©´ í†µê³¼ (ê¸°ì¡´: 100%)
        return valid_sections >= len(content_sections) * 0.5

    def _is_valid_original_section(self, section: Dict) -> bool:
        """ì›ë³¸ ë°ì´í„° ê¸°ë°˜ ì„¹ì…˜ ìœ íš¨ì„± ê²€ì¦ (ì™„í™”ëœ ê¸°ì¤€)"""
        if not isinstance(section, dict):
            return False
        
        # í´ë°± ë°ì´í„° ì œì™¸ (ìœ ì§€)
        metadata = section.get("metadata", {})
        if metadata.get("fallback_used"):
            return False
        
        # Azure AI Search í‚¤ì›Œë“œ ì°¨ë‹¨ (ìœ ì§€í•˜ë˜ ë” ê´€ëŒ€í•˜ê²Œ)
        azure_search_keywords = [
            "ë„ì‹œì˜ ë¯¸í•™", "ê³¨ëª©ê¸¸ì˜ ì¬ë°œê²¬", "ì•„í‹°ìŠ¤íŠ¸ ì¸í„°ë·°"  # í‚¤ì›Œë“œ ìˆ˜ ê°ì†Œ
        ]
        
        title = section.get("title", "").lower()
        body = section.get("body", "").lower()
        
        # í‚¤ì›Œë“œê°€ ì „ì²´ ë‚´ìš©ì˜ 30% ì´ìƒì„ ì°¨ì§€í•  ë•Œë§Œ ì œì™¸ (ê¸°ì¡´: í¬í•¨ë˜ë©´ ë¬´ì¡°ê±´ ì œì™¸)
        contamination_ratio = 0
        total_words = len((title + " " + body).split())
        
        for keyword in azure_search_keywords:
            if keyword in title or keyword in body:
                contamination_ratio += len(keyword.split()) / max(total_words, 1)
        
        if contamination_ratio > 0.3:  # 30% ì´ìƒ ì˜¤ì—¼ì‹œì—ë§Œ ì œì™¸
            print(f"ğŸš« Azure AI Search í‚¤ì›Œë“œ ì˜¤ì—¼ìœ¨ {contamination_ratio:.2f} ì´ˆê³¼, ì„¹ì…˜ ì œì™¸")
            return False
        
        # ìµœì†Œ ì½˜í…ì¸  ìš”êµ¬ì‚¬í•­ (ëŒ€í­ ì™„í™”)
        has_meaningful_content = (
            len(section.get("title", "")) > 0 or  # ì œëª©ì´ ìˆê±°ë‚˜
            len(section.get("body", "")) > 10 or  # 10ì ì´ìƒ ë³¸ë¬¸ì´ ìˆê±°ë‚˜
            len(section.get("subtitle", "")) > 0 or  # ë¶€ì œëª©ì´ ìˆê±°ë‚˜
            len(section.get("images", [])) > 0  # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ìœ íš¨
        )
        
        return has_meaningful_content


    async def _log_coordination_result_async(self, result: Dict, text_mapping: Dict, 
                                           image_distribution: Dict, org_results: List[Dict], 
                                           binding_results: List[Dict]):
        """ë¹„ë™ê¸° ì¡°ìœ¨ ê²°ê³¼ ë¡œê¹…"""
        try:
            response_id = self.logger.log_agent_real_output(
                agent_name="CoordinatorAgent",
                agent_role="ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ì",
                task_description=f"ë°°ì¹˜ ëª¨ë“œë¡œ {len(result.get('content_sections', []))}ê°œ ì„¹ì…˜ ìƒì„±",
                final_answer=str(result),
                reasoning_process="ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ ì•ˆì „í•œ ë§¤ê±°ì§„ êµ¬ì¡° í†µí•©",
                execution_steps=[
                    "ì´ì „ ê²°ê³¼ ë°°ì¹˜ ìˆ˜ì§‘",
                    "ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ",
                    "CrewAI ë°°ì¹˜ ì‹¤í–‰",
                    "ê²°ê³¼ í†µí•© ë° ê²€ì¦",
                    "í’ˆì§ˆ ë³´ì¦"
                ],
                raw_input={
                    "text_mapping": str(text_mapping)[:500],
                    "image_distribution": str(image_distribution)[:500]
                },
                raw_output=result,
                performance_metrics={
                    "batch_mode_used": True,
                    "total_sections": len(result.get('content_sections', [])),
                    "org_results_utilized": len(org_results),
                    "binding_results_utilized": len(binding_results),
                    "execution_stats": self.execution_stats,
                    "quality_score": result.get('integration_metadata', {}).get('integration_quality_score', 0),
                    "real_data_used": True
                }
            )
            
            result["final_response_id"] = response_id
            result["execution_mode"] = "batch_async"
            
        except Exception as e:
            print(f"âš ï¸ ë¹„ë™ê¸° ë¡œê¹… ì‹¤íŒ¨: {e}")


    async def _log_coordination_result_safe(self, result: Dict, text_mapping: Dict,
                                       image_distribution: Dict, org_results: List[Dict],
                                       binding_results: List[Dict]):
        """ì•ˆì „í•œ ì¡°ìœ¨ ê²°ê³¼ ë¡œê¹…"""
        try:
            # ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            session_metadata = {}
            if hasattr(self, 'current_session_id'):
                session_metadata = {
                    "session_id": self.current_session_id,
                    "agent_name": self.agent_name if hasattr(self, 'agent_name') else "CoordinatorAgent",
                    "isolation_applied": hasattr(self, 'isolation_manager'),
                    "communication_isolated": hasattr(self, 'communication_isolator')
                }
            
            # ì•ˆì „í•œ ë¡œê¹… ì‹¤í–‰
            response_id = self.logger.log_agent_real_output(
                agent_name="CoordinatorAgent",
                agent_role="ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ì",
                task_description=f"ë°°ì¹˜ ëª¨ë“œë¡œ {len(result.get('content_sections', []))}ê°œ ì„¹ì…˜ ìƒì„±",
                final_answer=str(result)[:1000],  # ê¸¸ì´ ì œí•œ
                reasoning_process="ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ ì•ˆì „í•œ ë§¤ê±°ì§„ êµ¬ì¡° í†µí•©",
                execution_steps=[
                    "ì´ì „ ê²°ê³¼ ë°°ì¹˜ ìˆ˜ì§‘",
                    "ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ", 
                    "CrewAI ë°°ì¹˜ ì‹¤í–‰",
                    "ê²°ê³¼ í†µí•© ë° ê²€ì¦",
                    "í’ˆì§ˆ ë³´ì¦"
                ],
                raw_input={
                    "text_mapping_summary": f"í…ìŠ¤íŠ¸ ë§¤í•‘ {len(str(text_mapping))} ë¬¸ì",
                    "image_distribution_summary": f"ì´ë¯¸ì§€ ë¶„ë°° {len(str(image_distribution))} ë¬¸ì",
                    "session_metadata": session_metadata
                },
                raw_output=result,
                performance_metrics={
                    "batch_mode_used": True,
                    "total_sections": len(result.get('content_sections', [])),
                    "org_results_utilized": len(org_results),
                    "binding_results_utilized": len(binding_results),
                    "execution_stats": getattr(self, 'execution_stats', {}),
                    "quality_score": result.get('integration_metadata', {}).get('integration_quality_score', 0),
                    "real_data_used": True,
                    "session_isolated": hasattr(self, 'current_session_id')
                }
            )
            
            # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result["final_response_id"] = response_id
            result["execution_mode"] = "batch_async_safe"
            result["session_metadata"] = session_metadata
            
        except Exception as e:
            print(f"âš ï¸ ì•ˆì „í•œ ë¡œê¹… ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ë©”íƒ€ë°ì´í„°ë¼ë„ ì¶”ê°€
            result["final_response_id"] = f"safe_fallback_{int(__import__('time').time())}"
            result["execution_mode"] = "safe_fallback"
            result["logging_error"] = str(e)

    def _load_results_from_file(self) -> List[Dict]:
        """íŒŒì¼ì—ì„œ ê²°ê³¼ ë¡œë“œ (ê°œì„ ë¨)"""
        try:
            # ì—¬ëŸ¬ íŒŒì¼ ê²½ë¡œ ì‹œë„
            possible_files = [
                "./output/agent_results.json",
                "./output/latest_outputs.json",
                "./output/sessions/session_data.json"
            ]
            
            for results_file in possible_files:
                if __import__('os').path.exists(results_file):
                    try:
                        with open(results_file, 'r', encoding='utf-8') as f:
                            data = __import__('json').load(f)
                        
                        # ë°ì´í„° í˜•ì‹ì— ë”°ë¼ ì²˜ë¦¬
                        if isinstance(data, list):
                            return data
                        elif isinstance(data, dict):
                            if 'latest_outputs' in data:
                                return data['latest_outputs']
                            elif 'agent_results' in data:
                                return data['agent_results']
                            else:
                                return [data]
                    except Exception as e:
                        print(f"âš ï¸ íŒŒì¼ {results_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
            
            return []
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []






    def get_execution_stats(self) -> Dict:
        """ì‹¤í–‰ í†µê³„ ë°˜í™˜"""
        return {
            **self.execution_stats,
            "success_rate": (self.execution_stats["successful_executions"] / 
                           max(self.execution_stats["total_attempts"], 1)) * 100,
            "fallback_rate": (self.execution_stats["fallback_used"] / 
                            max(self.execution_stats["total_attempts"], 1)) * 100,
            "circuit_breaker_state": self.circuit_breaker.state,
            "current_mode": "sync" if self.fallback_to_sync else "async"
        }

    def reset_execution_state(self):
        """ì‹¤í–‰ ìƒíƒœ ì´ˆê¸°í™”"""
        self.fallback_to_sync = False
        self.circuit_breaker = CircuitBreaker()
        self.execution_stats = {
            "total_attempts": 0,
            "successful_executions": 0,
            "fallback_used": 0,
            "circuit_breaker_triggered": 0,
            "timeout_occurred": 0
        }
        print("âœ… CoordinatorAgent ì‹¤í–‰ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.work_queue.executor:
            self.work_queue.executor.shutdown(wait=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        if exc_type:
            print(f"âš ï¸ CoordinatorAgent ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {exc_type.__name__}: {exc_val}")
            return False  # ì˜ˆì™¸ë¥¼ ì¬ë°œìƒì‹œí‚´
        
        return True

    def __enter__(self):
        """ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.work_queue.executor:
            self.work_queue.executor.shutdown(wait=True)
        
        if exc_type:
            print(f"âš ï¸ CoordinatorAgent ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {exc_type.__name__}: {exc_val}")
        
        return False

    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self.work_queue, 'executor') and self.work_queue.executor:
                self.work_queue.executor.shutdown(wait=True)
                print("âœ… ThreadPoolExecutor ì •ë¦¬ ì™„ë£Œ")
            
            # í ì •ë¦¬
            self.work_queue.work_queue.clear()
            self.work_queue.active_tasks.clear()
            self.work_queue.results.clear()
            
            print("âœ… CoordinatorAgent ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def health_check(self) -> Dict[str, Any]:
        """CoordinatorAgent ìƒíƒœ í™•ì¸"""
        try:
            # ê¸°ë³¸ ìƒíƒœ ì •ë³´
            health_status = {
                "status": "healthy",
                "timestamp": time.time(),
                "execution_mode": "sync" if self.fallback_to_sync else "async",
                "circuit_breaker_state": self.circuit_breaker.state,
                "queue_size": len(self.work_queue.work_queue),
                "active_tasks": len(self.work_queue.active_tasks),
                "execution_stats": self.execution_stats
            }
            
            # LLM ì—°ê²° í™•ì¸
            try:
                if self.llm:
                    health_status["llm_status"] = "connected"
                else:
                    health_status["llm_status"] = "disconnected"
                    health_status["status"] = "degraded"
            except Exception as e:
                health_status["llm_status"] = f"error: {str(e)}"
                health_status["status"] = "degraded"
            
            # ë¡œê±° ìƒíƒœ í™•ì¸
            try:
                if self.logger:
                    health_status["logger_status"] = "connected"
                else:
                    health_status["logger_status"] = "disconnected"
                    health_status["status"] = "degraded"
            except Exception as e:
                health_status["logger_status"] = f"error: {str(e)}"
                health_status["status"] = "degraded"
            
            # ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸
            agents_status = {}
            for agent_name, agent in [
                ("crew_agent", self.crew_agent),
                ("text_analyzer_agent", self.text_analyzer_agent),
                ("image_analyzer_agent", self.image_analyzer_agent)
            ]:
                try:
                    if agent and hasattr(agent, 'role'):
                        agents_status[agent_name] = "initialized"
                    else:
                        agents_status[agent_name] = "not_initialized"
                        health_status["status"] = "degraded"
                except Exception as e:
                    agents_status[agent_name] = f"error: {str(e)}"
                    health_status["status"] = "degraded"
            
            health_status["agents_status"] = agents_status
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (ì„ íƒì )
            try:
                import psutil
                process = psutil.Process()
                memory_info = process.memory_info()
                health_status["memory_usage"] = {
                    "rss": memory_info.rss,
                    "vms": memory_info.vms,
                    "percent": process.memory_percent()
                }
            except ImportError:
                health_status["memory_usage"] = "psutil not available"
            except Exception as e:
                health_status["memory_usage"] = f"error: {str(e)}"
            
            return health_status
            
        except Exception as e:
            return {
                "status": "error",
                "timestamp": time.time(),
                "error": str(e),
                "execution_stats": self.execution_stats
            }

    async def force_reset(self):
        """ê°•ì œ ì¬ì„¤ì •"""
        print("ğŸ”„ CoordinatorAgent ê°•ì œ ì¬ì„¤ì • ì‹œì‘")
        
        try:
            # 1. ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ì¤‘ë‹¨
            for task_id, task in self.work_queue.active_tasks.items():
                if not task.done():
                    task.cancel()
                    print(f"â¹ï¸ ì‘ì—… {task_id} ì·¨ì†Œ")
            
            # 2. í ë° ê²°ê³¼ ì •ë¦¬
            self.work_queue.work_queue.clear()
            self.work_queue.active_tasks.clear()
            self.work_queue.results.clear()
            
            # 3. ì‹¤í–‰ ìƒíƒœ ì´ˆê¸°í™”
            self.reset_execution_state()
            
            # 4. ì—ì´ì „íŠ¸ ì¬ìƒì„±
            self.crew_agent = self._create_crew_agent()
            self.text_analyzer_agent = self._create_text_analyzer_agent()
            self.image_analyzer_agent = self._create_image_analyzer_agent()
            
            print("âœ… CoordinatorAgent ê°•ì œ ì¬ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ê°•ì œ ì¬ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        total_attempts = self.execution_stats["total_attempts"]
        
        if total_attempts == 0:
            return {
                "success_rate": 0.0,
                "failure_rate": 0.0,
                "fallback_rate": 0.0,
                "timeout_rate": 0.0,
                "circuit_breaker_rate": 0.0,
                "total_attempts": 0,
                "current_mode": "sync" if self.fallback_to_sync else "async",
                "circuit_breaker_state": self.circuit_breaker.state
            }
        
        return {
            "success_rate": (self.execution_stats["successful_executions"] / total_attempts) * 100,
            "failure_rate": ((total_attempts - self.execution_stats["successful_executions"]) / total_attempts) * 100,
            "fallback_rate": (self.execution_stats["fallback_used"] / total_attempts) * 100,
            "timeout_rate": (self.execution_stats["timeout_occurred"] / total_attempts) * 100,
            "circuit_breaker_rate": (self.execution_stats["circuit_breaker_triggered"] / total_attempts) * 100,
            "total_attempts": total_attempts,
            "successful_executions": self.execution_stats["successful_executions"],
            "current_mode": "sync" if self.fallback_to_sync else "async",
            "circuit_breaker_state": self.circuit_breaker.state,
            "queue_utilization": len(self.work_queue.work_queue) / self.work_queue.max_queue_size * 100,
            "active_tasks_count": len(self.work_queue.active_tasks)
        }

    async def test_coordination_pipeline(self) -> Dict[str, Any]:
        """ì¡°ìœ¨ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        print("ğŸ§ª CoordinatorAgent íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = {
            "test_timestamp": time.time(),
            "tests_passed": 0,
            "tests_failed": 0,
            "test_details": []
        }
        
        # í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ ì´ˆê¸°í™” í™•ì¸
        try:
            assert self.llm is not None, "LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
            assert self.logger is not None, "Loggerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
            assert self.crew_agent is not None, "Crew Agentê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
            
            test_results["tests_passed"] += 1
            test_results["test_details"].append({
                "test_name": "initialization_test",
                "status": "passed",
                "message": "ëª¨ë“  êµ¬ì„± ìš”ì†Œê°€ ì •ìƒì ìœ¼ë¡œ ì´ˆê¸°í™”ë¨"
            })
        except Exception as e:
            test_results["tests_failed"] += 1
            test_results["test_details"].append({
                "test_name": "initialization_test",
                "status": "failed",
                "error": str(e)
            })
        
        # í…ŒìŠ¤íŠ¸ 2: ê°„ë‹¨í•œ ì‘ì—… ì‹¤í–‰ í…ŒìŠ¤íŠ¸
        try:
            test_task_result = await self.execute_with_resilience(
                task_func=lambda: {"test": "success"},
                task_id="pipeline_test",
                timeout=30.0,
                max_retries=1
            )
            
            assert test_task_result is not None, "í…ŒìŠ¤íŠ¸ ì‘ì—… ê²°ê³¼ê°€ None"
            
            test_results["tests_passed"] += 1
            test_results["test_details"].append({
                "test_name": "task_execution_test",
                "status": "passed",
                "message": "ì‘ì—… ì‹¤í–‰ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë¨",
                "result": test_task_result
            })
        except Exception as e:
            test_results["tests_failed"] += 1
            test_results["test_details"].append({
                "test_name": "task_execution_test",
                "status": "failed",
                "error": str(e)
            })
        
        # í…ŒìŠ¤íŠ¸ 3: ë°ì´í„° ì¶”ì¶œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        try:
            test_text_data = {
                "sections": [{
                    "template": "Section01.jsx",
                    "title": "í…ŒìŠ¤íŠ¸ ì œëª©",
                    "body": "í…ŒìŠ¤íŠ¸ ë³¸ë¬¸ ë‚´ìš©"
                }]
            }
            
            test_image_data = {
                "template_images": {
                    "Section01.jsx": ["https://example.com/test.jpg"]
                }
            }
            
            enhanced_structure = self._create_enhanced_structure(
                test_text_data, test_image_data, [], []
            )
            
            assert isinstance(enhanced_structure, dict), "êµ¬ì¡° ìƒì„± ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜"
            assert "content_sections" in enhanced_structure, "content_sections í‚¤ê°€ ì—†ìŒ"
            
            test_results["tests_passed"] += 1
            test_results["test_details"].append({
                "test_name": "data_extraction_test",
                "status": "passed",
                "message": "ë°ì´í„° ì¶”ì¶œ ë° êµ¬ì¡° ìƒì„±ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë¨"
            })
        except Exception as e:
            test_results["tests_failed"] += 1
            test_results["test_details"].append({
                "test_name": "data_extraction_test",
                "status": "failed",
                "error": str(e)
            })
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        total_tests = test_results["tests_passed"] + test_results["tests_failed"]
        test_results["success_rate"] = (test_results["tests_passed"] / total_tests * 100) if total_tests > 0 else 0
        test_results["overall_status"] = "passed" if test_results["tests_failed"] == 0 else "failed"
        
        print(f"ğŸ§ª íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_results['tests_passed']}/{total_tests} í†µê³¼")
        
        return test_results

# ì‚¬ìš© ì˜ˆì‹œ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_coordinator_agent() -> CoordinatorAgent:
    """CoordinatorAgent ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    try:
        coordinator = CoordinatorAgent()
        print("âœ… CoordinatorAgent ìƒì„± ì™„ë£Œ")
        return coordinator
    except Exception as e:
        print(f"âŒ CoordinatorAgent ìƒì„± ì‹¤íŒ¨: {e}")
        raise


async def run_coordination_with_monitoring(coordinator: CoordinatorAgent, 
                                         text_mapping: Dict, 
                                         image_distribution: Dict) -> Dict:
    """ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ì¡°ìœ¨ ì‹¤í–‰"""
    start_time = time.time()
    
    try:
        # ìƒíƒœ í™•ì¸
        health_status = await coordinator.health_check()
        if health_status["status"] == "error":
            print(f"âš ï¸ CoordinatorAgent ìƒíƒœ ë¶ˆëŸ‰: {health_status}")
        
        # ì¡°ìœ¨ ì‹¤í–‰
        result = await coordinator.coordinate_magazine_creation(text_mapping, image_distribution)
        
        # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
        execution_time = time.time() - start_time
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€
        result["execution_metadata"] = {
            "execution_time": execution_time,
            "performance_metrics": coordinator.get_performance_metrics(),
            "health_status": health_status
        }
        
        print(f"âœ… ì¡°ìœ¨ ì™„ë£Œ (ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ)")
        return result
        
    except Exception as e:
        execution_time = time.time() - start_time
        print(f"âŒ ì¡°ìœ¨ ì‹¤í–‰ ì‹¤íŒ¨ (ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ): {e}")
        
        # ì˜¤ë¥˜ ì •ë³´ì™€ í•¨ê»˜ í´ë°± ê²°ê³¼ ë°˜í™˜
        return {
            "selected_templates": ["Section01.jsx"],
            "content_sections": [{
                "template": "Section01.jsx",
                "title": "ë§¤ê±°ì§„ ìƒì„± ì˜¤ë¥˜",
                "subtitle": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ í´ë°±",
                "body": f"ì¡°ìœ¨ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "tagline": "SYSTEM ERROR",
                "images": [],
                "metadata": {
                    "error_fallback": True,
                    "error_message": str(e),
                    "execution_time": execution_time
                }
            }],
            "integration_metadata": {
                "total_sections": 1,
                "error_occurred": True,
                "execution_time": execution_time,
                "performance_metrics": coordinator.get_performance_metrics()
            }
        }

# ëª¨ë“ˆ ìˆ˜ì¤€ ìœ í‹¸ë¦¬í‹°
def validate_coordination_inputs(text_mapping: Dict, image_distribution: Dict) -> bool:
    """ì¡°ìœ¨ ì…ë ¥ ë°ì´í„° ê²€ì¦"""
    try:
        # text_mapping ê²€ì¦
        if not isinstance(text_mapping, dict):
            print("âš ï¸ text_mappingì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜")
            return False
        
        # image_distribution ê²€ì¦
        if not isinstance(image_distribution, dict):
            print("âš ï¸ image_distributionì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜")
            return False
        
        print("âœ… ì¡°ìœ¨ ì…ë ¥ ë°ì´í„° ê²€ì¦ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

# ì „ì—­ ì„¤ì •
COORDINATOR_CONFIG = {
    "max_workers": 1,
    "max_queue_size": 20,
    "default_timeout": 300.0,
    "max_retries": 2,
    "circuit_breaker_threshold": 5,
    "circuit_breaker_timeout": 60.0,
    "batch_size": 2,
    "recursion_threshold": 800
}

def update_coordinator_config(**kwargs):
    """CoordinatorAgent ì„¤ì • ì—…ë°ì´íŠ¸"""
    global COORDINATOR_CONFIG
    COORDINATOR_CONFIG.update(kwargs)
    print(f"âœ… CoordinatorAgent ì„¤ì • ì—…ë°ì´íŠ¸: {kwargs}")

# ëª¨ë“ˆ ì´ˆê¸°í™” ì‹œ ì‹¤í–‰ë˜ëŠ” ì½”ë“œ
if __name__ == "__main__":
    print("ğŸš€ CoordinatorAgent ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
    print(f"ğŸ“‹ í˜„ì¬ ì„¤ì •: {COORDINATOR_CONFIG}")
